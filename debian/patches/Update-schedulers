Description: <short summary of the patch>
 TODO: Put a short summary on the line above and replace this paragraph
 with a longer explanation of this change. Complete the meta-information
 with other relevant fields (see below for details). To make it easier, the
 information below has been extracted from the changelog. Adjust it or drop
 it.
 .
 scx (1.0.4.1-1ubuntu1) noble; urgency=medium
 .
   * Miscellaneous Ubuntu changes:
     - Rebase on top of the latest upstream main branch
Author: Mitchell Augustin <mitchell.augustin@canonical.com>

---
The information above should follow the Patch Tagging Guidelines, please
checkout https://dep.debian.net/deps/dep3/ to learn about the format. Here
are templates for supplementary fields that you might want to add:

Origin: (upstream|backport|vendor|other), (<patch-url>|commit:<commit-id>)
Bug: <upstream-bugtracker-url>
Bug-Debian: https://bugs.debian.org/<bugnumber>
Bug-Ubuntu: https://launchpad.net/bugs/<bugnumber>
Forwarded: (no|not-needed|<patch-forwarded-url>)
Applied-Upstream: <version>, (<commit-url>|commit:<commid-id>)
Reviewed-By: <name and email of someone who approved/reviewed the patch>
Last-Update: 2024-09-23

--- scx-1.0.4.1.orig/meson.build
+++ scx-1.0.4.1/meson.build
@@ -276,9 +276,9 @@ if get_option('buildtype') == 'release'
   cargo_build_args += '--release'
 endif
 
-#if get_option('offline')
-#  cargo_build_args += '--offline'
-#endif
+if get_option('offline')
+  cargo_build_args += '--offline'
+endif
 
 if get_option('kernel') != ''
   kernel = get_option('kernel')
--- /dev/null
+++ scx-1.0.4.1/scheds/rust/scx_rlfifo/intf.h
@@ -0,0 +1,106 @@
+// This software may be used and distributed according to the terms of the
+// GNU General Public License version 2.
+
+#ifndef __INTF_H
+#define __INTF_H
+
+#define MAX(x, y) ((x) > (y) ? (x) : (y))
+#define MIN(x, y) ((x) < (y) ? (x) : (y))
+
+#define NSEC_PER_SEC	1000000000L
+#define CLOCK_BOOTTIME	7
+
+#include <stdbool.h>
+#ifndef __kptr
+#ifdef __KERNEL__
+#error "__kptr_ref not defined in the kernel"
+#endif
+#define __kptr
+#endif
+
+#ifndef __VMLINUX_H__
+typedef unsigned char u8;
+typedef unsigned short u16;
+typedef unsigned int u32;
+typedef unsigned long u64;
+
+typedef signed char s8;
+typedef signed short s16;
+typedef signed int s32;
+typedef signed long s64;
+
+typedef int pid_t;
+#endif /* __VMLINUX_H__ */
+
+/* Check a condition at build time */
+#define BUILD_BUG_ON(expr) \
+	do { \
+		extern char __build_assert__[(expr) ? -1 : 1] \
+			__attribute__((unused)); \
+	} while(0)
+
+/*
+ * Maximum amount of CPUs supported by this scheduler (this defines the size of
+ * cpu_map that is used to store the idle state and CPU ownership).
+ */
+#define MAX_CPUS 1024
+
+/* Special dispatch flags */
+enum {
+	/*
+	 * Do not assign any specific CPU to the task.
+	 *
+	 * The task will be dispatched to the global shared DSQ and it will run
+	 * on the first CPU available.
+	 */
+	RL_CPU_ANY = 1 << 20,
+};
+
+/*
+ * Specify a target CPU for a specific PID.
+ */
+struct task_cpu_arg {
+	pid_t pid;
+	s32 cpu;
+	u64 flags;
+};
+
+/*
+ * Specify a sibling CPU relationship for a specific scheduling domain.
+ */
+struct domain_arg {
+	s32 lvl_id;
+	s32 cpu_id;
+	s32 sibling_cpu_id;
+};
+
+/*
+ * Task sent to the user-space scheduler by the BPF dispatcher.
+ *
+ * All attributes are collected from the kernel by the the BPF component.
+ */
+struct queued_task_ctx {
+	s32 pid;
+	s32 cpu; /* CPU where the task is running */
+	u64 flags; /* task enqueue flags */
+	u64 cpumask_cnt; /* cpumask generation counter */
+	u64 sum_exec_runtime; /* Total cpu time */
+	u64 weight; /* Task static priority */
+};
+
+/*
+ * Task sent to the BPF dispatcher by the user-space scheduler.
+ *
+ * This struct can be easily extended to send more information to the
+ * dispatcher (i.e., a target CPU, a variable time slice, etc.).
+ */
+struct dispatched_task_ctx {
+	s32 pid;
+	s32 cpu; /* CPU where the task should be dispatched */
+	u64 flags; /* task enqueue flags */
+	u64 slice_ns; /* time slice assigned to the task (0=default) */
+	u64 vtime; /* task deadline / vruntime */
+	u64 cpumask_cnt; /* cpumask generation counter */
+};
+
+#endif /* __INTF_H */
--- /dev/null
+++ scx-1.0.4.1/scheds/rust/scx_rlfifo/main.bpf.c
@@ -0,0 +1,1418 @@
+/* Copyright (c) Andrea Righi <andrea.righi@linux.dev> */
+/*
+ * scx_rustland_core: BPF backend for schedulers running in user-space.
+ *
+ * This BPF backend implements the low level sched-ext functionalities for a
+ * user-space counterpart, that implements the actual scheduling policy.
+ *
+ * The BPF part collects total cputime and weight from the tasks that need to
+ * run, then it sends all details to the user-space scheduler that decides the
+ * best order of execution of the tasks (based on the collected metrics).
+ *
+ * The user-space scheduler then returns to the BPF component the list of tasks
+ * to be dispatched in the proper order.
+ *
+ * Messages between the BPF component and the user-space scheduler are passed
+ * using BPF_MAP_TYPE_RINGBUFFER / BPF_MAP_TYPE_USER_RINGBUF maps: @queued for
+ * the messages sent by the BPF dispatcher to the user-space scheduler and
+ * @dispatched for the messages sent by the user-space scheduler to the BPF
+ * dispatcher.
+ *
+ * The BPF dispatcher is completely agnostic of the particular scheduling
+ * policy implemented in user-space. For this reason developers that are
+ * willing to use this scheduler to experiment scheduling policies should be
+ * able to simply modify the Rust component, without having to deal with any
+ * internal kernel / BPF details.
+ *
+ * This software may be used and distributed according to the terms of the
+ * GNU General Public License version 2.
+ */
+#include <scx/common.bpf.h>
+#include "intf.h"
+
+char _license[] SEC("license") = "GPL";
+
+UEI_DEFINE(uei);
+
+/*
+ * Introduce a custom DSQ shared across all the CPUs, where we can dispatch
+ * tasks that will be executed on the first CPU available.
+ *
+ * Per-CPU DSQs are also provided, to allow the scheduler to run a task on a
+ * specific CPU (see dsq_init()).
+ */
+#define SHARED_DSQ MAX_CPUS
+
+/*
+ * Scheduler attributes and statistics.
+ */
+u32 usersched_pid; /* User-space scheduler PID */
+const volatile bool switch_partial; /* Switch all tasks or SCHED_EXT tasks */
+
+/*
+ * Number of tasks that are queued for scheduling.
+ *
+ * This number is incremented by the BPF component when a task is queued to the
+ * user-space scheduler and it must be decremented by the user-space scheduler
+ * when a task is consumed.
+ */
+volatile u64 nr_queued;
+
+/*
+ * Number of tasks that are waiting for scheduling.
+ *
+ * This number must be updated by the user-space scheduler to keep track if
+ * there is still some scheduling work to do.
+ */
+volatile u64 nr_scheduled;
+
+/*
+ * Amount of currently running tasks.
+ */
+volatile u64 nr_running, nr_online_cpus;
+
+/* Dispatch statistics */
+volatile u64 nr_user_dispatches, nr_kernel_dispatches,
+	     nr_cancel_dispatches, nr_bounce_dispatches;
+
+/* Failure statistics */
+volatile u64 nr_failed_dispatches, nr_sched_congested;
+
+ /* Report additional debugging information */
+const volatile bool debug;
+
+/* Allow to use bpf_printk() only when @debug is set */
+#define dbg_msg(_fmt, ...) do {						\
+	if (debug)							\
+		bpf_printk(_fmt, ##__VA_ARGS__);			\
+} while(0)
+
+/*
+ * CPUs in the system have SMT is enabled.
+ */
+const volatile bool smt_enabled = true;
+
+/*
+ * Mask of offline CPUs, used to properly support CPU hotplugging.
+ */
+private(BPFLAND) struct bpf_cpumask __kptr *offline_cpumask;
+
+/*
+ * CPU hotplugging generation counter (used to notify the user-space
+ * counterpart when a CPU hotplug event happened, allowing it to refresh the
+ * topology information).
+ */
+volatile u64 cpu_hotplug_cnt;
+
+/*
+ * Set the state of a CPU in a cpumask.
+ */
+static bool set_cpu_state(struct bpf_cpumask *cpumask, s32 cpu, bool state)
+{
+	if (!cpumask)
+		return false;
+	if (state)
+		return bpf_cpumask_test_and_set_cpu(cpu, cpumask);
+	else
+		return bpf_cpumask_test_and_clear_cpu(cpu, cpumask);
+}
+
+/*
+ * Access a cpumask in read-only mode (typically to check bits).
+ */
+static const struct cpumask *cast_mask(struct bpf_cpumask *mask)
+{
+	return (const struct cpumask *)mask;
+}
+
+/*
+ * Allocate/re-allocate a new cpumask.
+ */
+static int calloc_cpumask(struct bpf_cpumask **p_cpumask)
+{
+	struct bpf_cpumask *cpumask;
+
+	cpumask = bpf_cpumask_create();
+	if (!cpumask)
+		return -ENOMEM;
+
+	cpumask = bpf_kptr_xchg(p_cpumask, cpumask);
+	if (cpumask)
+		bpf_cpumask_release(cpumask);
+
+	return 0;
+}
+
+/*
+ * Determine when we need to drain tasks dispatched to CPUs that went offline.
+ */
+static int offline_needed;
+
+/*
+ * Notify the scheduler that we need to drain and re-enqueue the tasks
+ * dispatched to the offline CPU DSQs.
+ */
+static void set_offline_needed(void)
+{
+	__sync_fetch_and_or(&offline_needed, 1);
+}
+
+/*
+ * Check and clear the state of the offline CPUs re-enqueuing.
+ */
+static bool test_and_clear_offline_needed(void)
+{
+	return __sync_fetch_and_and(&offline_needed, 0) == 1;
+}
+
+/*
+ * Maximum amount of tasks queued between kernel and user-space at a certain
+ * time.
+ *
+ * The @queued and @dispatched lists are used in a producer/consumer fashion
+ * between the BPF part and the user-space part.
+ */
+#define MAX_ENQUEUED_TASKS 4096
+
+/*
+ * Maximum amount of slots reserved to the tasks dispatched via shared queue.
+ */
+#define MAX_DISPATCH_SLOT (MAX_ENQUEUED_TASKS / 8)
+
+/*
+ * The map containing tasks that are queued to user space from the kernel.
+ *
+ * This map is drained by the user space scheduler.
+ */
+struct {
+	__uint(type, BPF_MAP_TYPE_RINGBUF);
+	__uint(max_entries, MAX_ENQUEUED_TASKS *
+				sizeof(struct queued_task_ctx));
+} queued SEC(".maps");
+
+/*
+ * The user ring buffer containing pids that are dispatched from user space to
+ * the kernel.
+ *
+ * Drained by the kernel in .dispatch().
+ */
+struct {
+        __uint(type, BPF_MAP_TYPE_USER_RINGBUF);
+	__uint(max_entries, MAX_ENQUEUED_TASKS *
+				sizeof(struct dispatched_task_ctx));
+} dispatched SEC(".maps");
+
+/*
+ * Per-CPU context.
+ */
+struct cpu_ctx {
+	struct bpf_cpumask __kptr *l2_cpumask;
+	struct bpf_cpumask __kptr *l3_cpumask;
+};
+
+struct {
+	__uint(type, BPF_MAP_TYPE_PERCPU_ARRAY);
+	__type(key, u32);
+	__type(value, struct cpu_ctx);
+	__uint(max_entries, 1);
+} cpu_ctx_stor SEC(".maps");
+
+/*
+ * Return a CPU context.
+ */
+struct cpu_ctx *try_lookup_cpu_ctx(s32 cpu)
+{
+	const u32 idx = 0;
+	return bpf_map_lookup_percpu_elem(&cpu_ctx_stor, &idx, cpu);
+}
+
+/*
+ * Per-task local storage.
+ *
+ * This contain all the per-task information used internally by the BPF code.
+ */
+struct task_ctx {
+	/*
+	 * Temporary cpumask for calculating scheduling domains.
+	 */
+	struct bpf_cpumask __kptr *l2_cpumask;
+	struct bpf_cpumask __kptr *l3_cpumask;
+
+	/*
+	 * Time slice assigned to the task.
+	 */
+	u64 slice_ns;
+
+	/*
+	 * cpumask generation counter: used to verify the validity of the
+	 * current task's cpumask.
+	 */
+	u64 cpumask_cnt;
+};
+
+/* Map that contains task-local storage. */
+struct {
+	__uint(type, BPF_MAP_TYPE_TASK_STORAGE);
+	__uint(map_flags, BPF_F_NO_PREALLOC);
+	__type(key, int);
+	__type(value, struct task_ctx);
+} task_ctx_stor SEC(".maps");
+
+/*
+ * Return a local task context from a generic task or NULL if the context
+ * doesn't exist.
+ */
+struct task_ctx *try_lookup_task_ctx(const struct task_struct *p)
+{
+	struct task_ctx *tctx = bpf_task_storage_get(&task_ctx_stor,
+						(struct task_struct *)p, 0, 0);
+	if (!tctx)
+		dbg_msg("warning: failed to get task context for pid=%d (%s)",
+			p->pid, p->comm);
+	return tctx;
+}
+
+/*
+ * Intercept when a task is executing sched_setaffinity().
+ */
+struct {
+	__uint(type, BPF_MAP_TYPE_HASH);
+	__type(key, __u32);
+	__type(value, __u64);
+	__uint(max_entries, MAX_ENQUEUED_TASKS);
+} pid_setaffinity_map SEC(".maps");
+
+SEC("kprobe/sched_setaffinity")
+int BPF_KPROBE(kprobe_sched_setaffinity, struct task_struct *task,
+			const struct cpumask *new_mask)
+{
+	pid_t pid = bpf_get_current_pid_tgid() >> 32;
+	u64 value = true;
+
+	bpf_map_update_elem(&pid_setaffinity_map, &pid, &value, BPF_ANY);
+
+	return 0;
+}
+
+SEC("kretprobe/sched_setaffinity")
+int BPF_KRETPROBE(kretprobe_sched_setaffinity)
+{
+	pid_t pid = bpf_get_current_pid_tgid() >> 32;
+	bpf_map_delete_elem(&pid_setaffinity_map, &pid);
+
+	return 0;
+}
+
+/*
+ * Return true if a task is executing sched_setaffinity(), false otherwise.
+ */
+static bool in_setaffinity(pid_t pid)
+{
+	u64 *value = bpf_map_lookup_elem(&pid_setaffinity_map, &pid);
+	return value != NULL;
+}
+
+/*
+ * Heartbeat timer used to periodically trigger the check to run the user-space
+ * scheduler.
+ *
+ * Without this timer we may starve the scheduler if the system is completely
+ * idle and hit the watchdog that would auto-kill this scheduler.
+ */
+struct usersched_timer {
+	struct bpf_timer timer;
+};
+
+struct {
+	__uint(type, BPF_MAP_TYPE_ARRAY);
+	__uint(max_entries, 1);
+	__type(key, u32);
+	__type(value, struct usersched_timer);
+} usersched_timer SEC(".maps");
+
+/*
+ * Time period of the scheduler heartbeat, used to periodically kick the
+ * user-space scheduler and check if there is any pending activity.
+ */
+#define USERSCHED_TIMER_NS (NSEC_PER_SEC / 10)
+
+/*
+ * Return true if the target task @p is the user-space scheduler.
+ */
+static inline bool is_usersched_task(const struct task_struct *p)
+{
+	return p->pid == usersched_pid;
+}
+
+/*
+ * Return true if the target task @p is a kernel thread.
+ */
+static inline bool is_kthread(const struct task_struct *p)
+{
+	return p->flags & PF_KTHREAD;
+}
+
+/*
+ * Flag used to wake-up the user-space scheduler.
+ */
+static volatile u32 usersched_needed;
+
+/*
+ * Set user-space scheduler wake-up flag (equivalent to an atomic release
+ * operation).
+ */
+static void set_usersched_needed(void)
+{
+	__sync_fetch_and_or(&usersched_needed, 1);
+}
+
+/*
+ * Check and clear user-space scheduler wake-up flag (equivalent to an atomic
+ * acquire operation).
+ */
+static bool test_and_clear_usersched_needed(void)
+{
+	return __sync_fetch_and_and(&usersched_needed, 0) == 1;
+}
+
+/*
+ * Return true if there's any pending activity to do for the scheduler, false
+ * otherwise.
+ *
+ * NOTE: nr_queued is incremented by the BPF component, more exactly in
+ * enqueue(), when a task is sent to the user-space scheduler, then the
+ * scheduler drains the queued tasks (updating nr_queued) and adds them to its
+ * internal data structures / state; at this point tasks become "scheduled" and
+ * the user-space scheduler will take care of updating nr_scheduled
+ * accordingly; lastly tasks will be dispatched and the user-space scheduler
+ * will update nr_scheduled again.
+ *
+ * Checking both counters allows to determine if there is still some pending
+ * work to do for the scheduler: new tasks have been queued since last check,
+ * or there are still tasks "queued" or "scheduled" since the previous
+ * user-space scheduler run. If the counters are both zero it is pointless to
+ * wake-up the scheduler (even if a CPU becomes idle), because there is nothing
+ * to do.
+ *
+ * Also keep in mind that we don't need any protection here since this code
+ * doesn't run concurrently with the user-space scheduler (that is single
+ * threaded), therefore this check is also safe from a concurrency perspective.
+ */
+static bool usersched_has_pending_tasks(void)
+{
+	return nr_queued || nr_scheduled;
+}
+
+/*
+ * Return the corresponding CPU associated to a DSQ.
+ */
+static s32 dsq_to_cpu(u64 dsq_id)
+{
+	if (dsq_id >= MAX_CPUS) {
+		scx_bpf_error("Invalid dsq_id: %llu", dsq_id);
+		return -EINVAL;
+	}
+	return (s32)dsq_id;
+}
+
+/*
+ * Return the DSQ ID associated to a CPU, or SHARED_DSQ if the CPU is not
+ * valid.
+ */
+static u64 cpu_to_dsq(s32 cpu)
+{
+	if (cpu < 0 || cpu >= MAX_CPUS) {
+		scx_bpf_error("Invalid cpu: %d", cpu);
+		return SHARED_DSQ;
+	}
+	return (u64)cpu;
+}
+
+/*
+ * Return the time slice assigned to the task.
+ */
+static inline u64 task_slice(struct task_struct *p)
+{
+	struct task_ctx *tctx;
+
+	tctx = try_lookup_task_ctx(p);
+	if (!tctx || !tctx->slice_ns)
+		return SCX_SLICE_DFL;
+	return tctx->slice_ns;
+}
+
+/*
+ * Find an idle CPU in the system for the task.
+ *
+ * NOTE: the idle CPU selection doesn't need to be formally perfect, it is
+ * totally fine to accept racy conditions and potentially make mistakes, by
+ * picking CPUs that are not idle or even offline, the logic has been designed
+ * to handle these mistakes in favor of a more efficient response and a reduced
+ * scheduling overhead.
+ */
+static s32 pick_idle_cpu(struct task_struct *p, s32 prev_cpu, u64 enq_flags)
+{
+	const struct cpumask *online_cpumask, *idle_smtmask, *idle_cpumask;
+	struct bpf_cpumask *l2_domain, *l3_domain;
+	struct bpf_cpumask *l2_mask, *l3_mask;
+	struct task_ctx *tctx;
+	struct cpu_ctx *cctx;
+	s32 cpu;
+
+	/*
+	 * If the task isn't allowed to use its previously used CPU it means
+	 * that it's rapidly changing affinity. In this case it's pointless to
+	 * find an optimal idle CPU, just return and let the task being
+	 * dispatched to a global DSQ.
+	 */
+	if (!bpf_cpumask_test_cpu(prev_cpu, p->cpus_ptr))
+		return -ENOENT;
+
+	/*
+	 * For tasks that can run only on a single CPU, simply return the only
+	 * allowed CPU.
+	 */
+	if (p->nr_cpus_allowed == 1)
+		return prev_cpu;
+
+	tctx = try_lookup_task_ctx(p);
+	if (!tctx)
+		return -ENOENT;
+
+	cctx = try_lookup_cpu_ctx(prev_cpu);
+	if (!cctx)
+		return -ENOENT;
+
+	/*
+	 * Acquire the CPU masks to determine the online and idle CPUs in the
+	 * system.
+	 */
+	online_cpumask = scx_bpf_get_online_cpumask();
+	idle_smtmask = scx_bpf_get_idle_smtmask();
+	idle_cpumask = scx_bpf_get_idle_cpumask();
+
+	/*
+	 * Scheduling domains of the previously used CPU.
+	 */
+	l2_domain = cctx->l2_cpumask;
+	if (!l2_domain)
+		l2_domain = (struct bpf_cpumask *)p->cpus_ptr;
+
+	l3_domain = cctx->l3_cpumask;
+	if (!l3_domain)
+		l3_domain = (struct bpf_cpumask *)p->cpus_ptr;
+
+	/*
+	 * Task's scheduling domains.
+	 */
+	l2_mask = tctx->l2_cpumask;
+	if (!l2_mask) {
+		scx_bpf_error("l2 cpumask not initialized");
+		cpu = -ENOENT;
+		goto out_put_cpumask;
+	}
+	l3_mask = tctx->l3_cpumask;
+	if (!l3_mask) {
+		scx_bpf_error("l3 cpumask not initialized");
+		cpu = -ENOENT;
+		goto out_put_cpumask;
+	}
+
+	/*
+	 * Determine the L2 cache domain as the intersection of the task's
+	 * primary cpumask and the L2 cache domain mask of the previously used
+	 * CPU (ignore if this cpumask completely overlaps with the task's
+	 * cpumask).
+	 */
+	bpf_cpumask_and(l2_mask, p->cpus_ptr, cast_mask(l2_domain));
+
+	/*
+	 * Determine the L3 cache domain as the intersection of the task's
+	 * primary cpumask and the L3 cache domain mask of the previously used
+	 * CPU (ignore if this cpumask completely overlaps with the task's
+	 * cpumask).
+	 */
+	bpf_cpumask_and(l3_mask, p->cpus_ptr, cast_mask(l3_domain));
+
+	if (enq_flags & SCX_ENQ_WAKEUP) {
+		struct task_struct *current = (void *)bpf_get_current_task_btf();
+		bool share_llc, has_idle;
+
+		/*
+		 * Determine waker CPU scheduling domain.
+		 */
+		cpu = bpf_get_smp_processor_id();
+
+		cctx = try_lookup_cpu_ctx(cpu);
+		if (!cctx) {
+			cpu = -ENOENT;
+			goto out_put_cpumask;
+		}
+
+		l3_domain = cctx->l3_cpumask;
+		if (!l3_domain) {
+			scx_bpf_error("CPU LLC cpumask not initialized");
+			cpu = -ENOENT;
+			goto out_put_cpumask;
+		}
+
+		/*
+		 * If both the waker and wakee share the same LLC keep using
+		 * the same CPU if possible.
+		 */
+		share_llc = bpf_cpumask_test_cpu(prev_cpu, cast_mask(l3_domain));
+		if (share_llc && scx_bpf_test_and_clear_cpu_idle(prev_cpu)) {
+			cpu = prev_cpu;
+			goto out_put_cpumask;
+		}
+
+		/*
+		 * If the waker's domain is not saturated attempt to migrate
+		 * the wakee on the same CPU as the waker.
+		 */
+		has_idle = bpf_cpumask_intersects(cast_mask(l3_domain), idle_cpumask);
+		if (has_idle &&
+		    bpf_cpumask_test_cpu(cpu, p->cpus_ptr) &&
+		    !(current->flags & PF_EXITING) &&
+		    scx_bpf_dsq_nr_queued(cpu_to_dsq(cpu)) == 0)
+			goto out_put_cpumask;
+	}
+
+	/*
+	 * Find the best idle CPU, prioritizing full idle cores in SMT systems.
+	 */
+	if (smt_enabled) {
+		/*
+		 * If the task can still run on the previously used CPU and
+		 * it's a full-idle core, keep using it.
+		 */
+		if (bpf_cpumask_test_cpu(prev_cpu, p->cpus_ptr) &&
+		    bpf_cpumask_test_cpu(prev_cpu, idle_smtmask) &&
+		    scx_bpf_test_and_clear_cpu_idle(prev_cpu)) {
+			cpu = prev_cpu;
+			goto out_put_cpumask;
+		}
+
+		/*
+		 * Search for any full-idle CPU in the task domain that shares
+		 * the same L2 cache.
+		 */
+		cpu = bpf_cpumask_any_and_distribute(cast_mask(l2_mask), idle_smtmask);
+		if (bpf_cpumask_test_cpu(cpu, online_cpumask) &&
+		    scx_bpf_test_and_clear_cpu_idle(cpu))
+			goto out_put_cpumask;
+
+		/*
+		 * Search for any full-idle CPU in the task domain that shares
+		 * the same L3 cache.
+		 */
+		cpu = bpf_cpumask_any_and_distribute(cast_mask(l3_mask), idle_smtmask);
+		if (bpf_cpumask_test_cpu(cpu, online_cpumask) &&
+		    scx_bpf_test_and_clear_cpu_idle(cpu))
+			goto out_put_cpumask;
+
+		/*
+		 * Otherwise, search for another usable full-idle core.
+		 */
+		cpu = bpf_cpumask_any_and_distribute(p->cpus_ptr, idle_smtmask);
+		if (bpf_cpumask_test_cpu(cpu, online_cpumask) &&
+		    scx_bpf_test_and_clear_cpu_idle(cpu))
+			goto out_put_cpumask;
+	}
+
+	/*
+	 * If a full-idle core can't be found (or if this is not an SMT system)
+	 * try to re-use the same CPU, even if it's not in a full-idle core.
+	 */
+	if (bpf_cpumask_test_cpu(prev_cpu, p->cpus_ptr) &&
+	    scx_bpf_test_and_clear_cpu_idle(prev_cpu)) {
+		cpu = prev_cpu;
+		goto out_put_cpumask;
+	}
+
+	/*
+	 * Search for any idle CPU in the primary domain that shares the same
+	 * L2 cache.
+	 */
+	cpu = bpf_cpumask_any_and_distribute(cast_mask(l2_mask), idle_cpumask);
+	if (bpf_cpumask_test_cpu(cpu, online_cpumask) &&
+	    scx_bpf_test_and_clear_cpu_idle(cpu))
+		goto out_put_cpumask;
+
+	/*
+	 * Search for any idle CPU in the primary domain that shares the same
+	 * L3 cache.
+	 */
+	cpu = bpf_cpumask_any_and_distribute(cast_mask(l3_mask), idle_cpumask);
+	if (bpf_cpumask_test_cpu(cpu, online_cpumask) &&
+	    scx_bpf_test_and_clear_cpu_idle(cpu))
+		goto out_put_cpumask;
+
+	/*
+	 * If all the previous attempts have failed, try to use any idle CPU in
+	 * the system.
+	 */
+	cpu = bpf_cpumask_any_and_distribute(p->cpus_ptr, idle_cpumask);
+	if (bpf_cpumask_test_cpu(cpu, online_cpumask) &&
+	    scx_bpf_test_and_clear_cpu_idle(cpu))
+		goto out_put_cpumask;
+
+	/*
+	 * If all the previous attempts have failed, dispatch the task to the
+	 * first CPU that will become available.
+	 */
+	cpu = -ENOENT;
+
+out_put_cpumask:
+	scx_bpf_put_cpumask(idle_cpumask);
+	scx_bpf_put_cpumask(idle_smtmask);
+	scx_bpf_put_cpumask(online_cpumask);
+
+	return cpu;
+}
+
+/*
+ * Dispatch a task to a target per-CPU DSQ, waking up the corresponding CPU, if
+ * needed.
+ */
+static void dispatch_task(const struct dispatched_task_ctx *task)
+{
+	struct task_struct *p;
+	struct task_ctx *tctx;
+	u64 dsq_id, curr_cpumask_cnt;
+	s32 cpu;
+
+	/* Ignore entry if the task doesn't exist anymore */
+	p = bpf_task_from_pid(task->pid);
+	if (!p)
+		return;
+
+	/*
+	 * Update task's time slice in its context.
+	 */
+	tctx = try_lookup_task_ctx(p);
+	if (!tctx) {
+		/*
+		 * Bounce to the shared DSQ if we can't find a valid task
+		 * context.
+		 */
+		scx_bpf_dispatch_vtime(p, SHARED_DSQ,
+				       SCX_SLICE_DFL, task->vtime, task->flags);
+		__sync_fetch_and_add(&nr_bounce_dispatches, 1);
+		goto out_kick_idle_cpu;
+	}
+	tctx->slice_ns = task->slice_ns;
+
+	/*
+	 * Dispatch task to the target DSQ.
+	 */
+	if (task->cpu & RL_CPU_ANY) {
+		scx_bpf_dispatch_vtime(p, SHARED_DSQ,
+				       SCX_SLICE_DFL, task->vtime, task->flags);
+		goto out_kick_idle_cpu;
+	}
+
+	/*
+	 * Force tasks that are currently executing sched_setaffinity() to be
+	 * dispatched on the shared DSQ, otherwise we may introduce stalls in
+	 * the per-CPU DSQ.
+	 */
+	if (in_setaffinity(p->pid)) {
+		scx_bpf_dispatch_vtime(p, SHARED_DSQ,
+				       SCX_SLICE_DFL, task->vtime, task->flags);
+		__sync_fetch_and_add(&nr_bounce_dispatches, 1);
+		goto out_kick_idle_cpu;
+	}
+
+	/*
+	 * Dispatch a task to a specific per-CPU DSQ if the target CPU can be
+	 * used (according to the cpumask), otherwise redirect the task to the
+	 * shared DSQ.
+	 *
+	 * This can happen if the user-space scheduler dispatches the task to
+	 * an invalid CPU. In this case the redirection to the shared DSQ
+	 * allows to prevent potential stalls in the scheduler.
+	 *
+	 * If the cpumask is not valid anymore (determined by the cpumask_cnt
+	 * generation counter) we can simply cancel the dispatch event, since
+	 * the task will be re-enqueued by the core sched-ext code, potentially
+	 * selecting a different cpu and a different cpumask.
+	 */
+	dsq_id = cpu_to_dsq(task->cpu);
+
+	/* Check if the CPU is valid, according to the cpumask */
+	if (!bpf_cpumask_test_cpu(task->cpu, p->cpus_ptr)) {
+		scx_bpf_dispatch_vtime(p, SHARED_DSQ,
+				       SCX_SLICE_DFL, task->vtime, task->flags);
+		__sync_fetch_and_add(&nr_bounce_dispatches, 1);
+		goto out_kick_idle_cpu;
+	}
+
+	/* Read current cpumask generation counter */
+	curr_cpumask_cnt = tctx->cpumask_cnt;
+
+	/* Dispatch the task to the target per-CPU DSQ */
+	scx_bpf_dispatch_vtime(p, dsq_id,
+			       SCX_SLICE_DFL, task->vtime, task->flags);
+
+	/* If the cpumask is not valid anymore, ignore the dispatch event */
+	if (curr_cpumask_cnt != task->cpumask_cnt) {
+		scx_bpf_dispatch_cancel();
+		__sync_fetch_and_add(&nr_cancel_dispatches, 1);
+		goto out_release;
+	}
+
+	if (task->cpu != bpf_get_smp_processor_id())
+		scx_bpf_kick_cpu(task->cpu, SCX_KICK_IDLE);
+
+	goto out_release;
+
+out_kick_idle_cpu:
+	cpu = pick_idle_cpu(p, task->cpu, task->flags);
+	if (cpu >= 0)
+		scx_bpf_kick_cpu(cpu, 0);
+
+out_release:
+	bpf_task_release(p);
+}
+
+s32 BPF_STRUCT_OPS(rustland_select_cpu, struct task_struct *p, s32 prev_cpu,
+		   u64 wake_flags)
+{
+	/*
+	 * Completely delegate the CPU selection logic to the user-space
+	 * scheduler.
+	 */
+	return prev_cpu;
+}
+
+/*
+ * Select an idle CPU for a specific task from the user-space scheduler.
+ */
+SEC("syscall")
+int rs_select_cpu(struct task_cpu_arg *input)
+{
+	struct task_struct *p;
+	int cpu;
+
+	p = bpf_task_from_pid(input->pid);
+	if (!p)
+		return -EINVAL;
+	bpf_rcu_read_lock();
+	cpu = pick_idle_cpu(p, input->cpu, input->flags);
+	bpf_rcu_read_unlock();
+
+	bpf_task_release(p);
+
+	return cpu;
+}
+
+/*
+ * Fill @task with all the information that need to be sent to the user-space
+ * scheduler.
+ */
+static void get_task_info(struct queued_task_ctx *task,
+			  const struct task_struct *p, u64 enq_flags)
+{
+	struct task_ctx *tctx = try_lookup_task_ctx(p);
+
+	task->pid = p->pid;
+	task->sum_exec_runtime = p->se.sum_exec_runtime;
+	task->flags = enq_flags;
+	task->weight = p->scx.weight;
+	task->cpu = scx_bpf_task_cpu(p);
+	task->cpumask_cnt = tctx ? tctx->cpumask_cnt : 0;
+}
+
+/*
+ * User-space scheduler is congested: log that and increment congested counter.
+ */
+static void sched_congested(struct task_struct *p)
+{
+	dbg_msg("congested: pid=%d (%s)", p->pid, p->comm);
+	__sync_fetch_and_add(&nr_sched_congested, 1);
+}
+
+/*
+ * Task @p becomes ready to run. We can dispatch the task directly here if the
+ * user-space scheduler is not required, or enqueue it to be processed by the
+ * scheduler.
+ */
+void BPF_STRUCT_OPS(rustland_enqueue, struct task_struct *p, u64 enq_flags)
+{
+	struct queued_task_ctx *task;
+
+	/*
+	 * Scheduler is dispatched directly in .dispatch() when needed, so
+	 * we can skip it here.
+	 */
+	if (is_usersched_task(p))
+		return;
+
+	/*
+	 * Always dispatch all kthreads directly on their target CPU.
+	 *
+	 * This allows to prevent critical kernel threads from being stuck in
+	 * user-space causing system hangs.
+	 */
+	if (is_kthread(p)) {
+		s32 cpu = scx_bpf_task_cpu(p);
+
+		scx_bpf_dispatch_vtime(p, cpu_to_dsq(cpu),
+				       SCX_SLICE_DFL, 0, enq_flags);
+		__sync_fetch_and_add(&nr_kernel_dispatches, 1);
+		return;
+	}
+
+	/*
+	 * Add tasks to the @queued list, they will be processed by the
+	 * user-space scheduler.
+	 *
+	 * If @queued list is full (user-space scheduler is congested) tasks
+	 * will be dispatched directly from the kernel (using the first CPU
+	 * available in this case).
+	 */
+	task = bpf_ringbuf_reserve(&queued, sizeof(*task), 0);
+	if (!task) {
+		sched_congested(p);
+		scx_bpf_dispatch_vtime(p, SHARED_DSQ,
+				       SCX_SLICE_DFL, 0, enq_flags);
+		__sync_fetch_and_add(&nr_kernel_dispatches, 1);
+		return;
+	}
+	get_task_info(task, p, enq_flags);
+	dbg_msg("enqueue: pid=%d (%s)", p->pid, p->comm);
+	bpf_ringbuf_submit(task, 0);
+
+	__sync_fetch_and_add(&nr_queued, 1);
+}
+
+/*
+ * Dispatch the user-space scheduler.
+ */
+static bool dispatch_user_scheduler(void)
+{
+	struct task_struct *p;
+	s32 cpu;
+
+	if (!test_and_clear_usersched_needed())
+		return false;
+
+	p = bpf_task_from_pid(usersched_pid);
+	if (!p) {
+		scx_bpf_error("Failed to find usersched task %d", usersched_pid);
+		return false;
+	}
+	/*
+	 * Use the highest vtime possible to give the scheduler itself the
+	 * lowest priority possible.
+	 */
+	scx_bpf_dispatch_vtime(p, SHARED_DSQ, SCX_SLICE_DFL, -1ULL, 0);
+
+	cpu = pick_idle_cpu(p, scx_bpf_task_cpu(p), 0);
+	if (cpu >= 0)
+		scx_bpf_kick_cpu(cpu, 0);
+	bpf_task_release(p);
+
+	return true;
+}
+
+/*
+ * Handle a task dispatched from user-space, performing the actual low-level
+ * BPF dispatch.
+ */
+static long handle_dispatched_task(struct bpf_dynptr *dynptr, void *context)
+{
+	const struct dispatched_task_ctx *task;
+
+	task = bpf_dynptr_data(dynptr, 0, sizeof(*task));
+	if (!task)
+		return 0;
+
+	dispatch_task(task);
+	__sync_fetch_and_add(&nr_user_dispatches, 1);
+
+	return !!scx_bpf_dispatch_nr_slots();
+}
+
+/*
+ * Consume tasks dispatched to CPUs that have gone offline.
+ *
+ * These tasks will be consumed on other active CPUs to prevent indefinite
+ * stalling.
+ *
+ * Return true if one task is consumed, false otherwise.
+ */
+static bool consume_offline_cpus(s32 cpu)
+{
+	u64 nr_cpu_ids = scx_bpf_nr_cpu_ids();
+	struct bpf_cpumask *offline;
+	bool ret = false;
+
+	if (!test_and_clear_offline_needed())
+		return false;
+
+	offline = offline_cpumask;
+	if (!offline)
+		return false;
+
+	/*
+	 * Cycle through all the CPUs and evenly consume tasks from the DSQs of
+	 * those that are offline.
+	 */
+	bpf_repeat(nr_cpu_ids - 1) {
+		cpu = (cpu + 1) % nr_cpu_ids;
+
+		if (!bpf_cpumask_test_cpu(cpu, cast_mask(offline)))
+			continue;
+		/*
+		 * This CPU is offline, if a task has been dispatched there
+		 * consume it immediately on the current CPU.
+		 */
+		if (scx_bpf_consume(cpu_to_dsq(cpu))) {
+			set_offline_needed();
+			ret = true;
+			break;
+		}
+	}
+
+	return ret;
+}
+
+/*
+ * Dispatch tasks that are ready to run.
+ *
+ * This function is called when a CPU's local DSQ is empty and ready to accept
+ * new dispatched tasks.
+ *
+ * We may dispatch tasks also on other CPUs from here, if the scheduler decided
+ * so (usually if other CPUs are idle we may want to send more tasks to their
+ * local DSQ to optimize the scheduling pipeline).
+ */
+void BPF_STRUCT_OPS(rustland_dispatch, s32 cpu, struct task_struct *prev)
+{
+	/*
+	 * Consume all tasks from the @dispatched list and immediately dispatch
+	 * them on the target CPU decided by the user-space scheduler.
+	 */
+	bpf_user_ringbuf_drain(&dispatched, handle_dispatched_task, NULL, 0);
+
+	/*
+	 * Try to steal a task dispatched to CPUs that may have gone offline
+	 * (this allows to prevent indefinite task stalls).
+	 */
+	if (consume_offline_cpus(cpu))
+		return;
+
+	/*
+	 * Consume a task from the per-CPU DSQ.
+	 */
+	if (scx_bpf_consume(cpu_to_dsq(cpu)))
+		return;
+
+	/*
+	 * Consume a task from the shared DSQ.
+	 */
+	if (scx_bpf_consume(SHARED_DSQ))
+		return;
+
+	/*
+	 * Check if the user-space scheduler needs to run.
+	 */
+	dispatch_user_scheduler();
+}
+
+/*
+ * Task @p starts on its selected CPU (update CPU ownership map).
+ */
+void BPF_STRUCT_OPS(rustland_running, struct task_struct *p)
+{
+	s32 cpu = scx_bpf_task_cpu(p);
+
+	dbg_msg("start: pid=%d (%s) cpu=%ld", p->pid, p->comm, cpu);
+
+	/*
+	 * Ensure time slice never exceeds slice_ns when a task is started on a
+	 * CPU.
+	 */
+	p->scx.slice = task_slice(p);
+
+	/*
+	 * Mark the CPU as busy by setting the pid as owner (ignoring the
+	 * user-space scheduler).
+	 */
+	if (!is_usersched_task(p))
+		__sync_fetch_and_add(&nr_running, 1);
+}
+
+/*
+ * Task @p stops running on its associated CPU (update CPU ownership map).
+ */
+void BPF_STRUCT_OPS(rustland_stopping, struct task_struct *p, bool runnable)
+{
+	s32 cpu = scx_bpf_task_cpu(p);
+
+	dbg_msg("stop: pid=%d (%s) cpu=%ld", p->pid, p->comm, cpu);
+	/*
+	 * Mark the CPU as idle by setting the owner to 0.
+	 */
+	if (!is_usersched_task(p)) {
+		__sync_fetch_and_sub(&nr_running, 1);
+		/*
+		 * Kick the user-space scheduler immediately when a task
+		 * releases a CPU and speculate on the fact that most of the
+		 * time there is another task ready to run.
+		 */
+		set_usersched_needed();
+	}
+}
+
+/*
+ * A CPU is about to change its idle state.
+ */
+void BPF_STRUCT_OPS(rustland_update_idle, s32 cpu, bool idle)
+{
+	/*
+	 * Don't do anything if we exit from and idle state, a CPU owner will
+	 * be assigned in .running().
+	 */
+	if (!idle)
+		return;
+	/*
+	 * A CPU is now available, notify the user-space scheduler that tasks
+	 * can be dispatched.
+	 */
+	if (usersched_has_pending_tasks()) {
+		set_usersched_needed();
+		/*
+		 * Wake up the idle CPU and trigger a resched, so that it can
+		 * immediately accept dispatched tasks.
+		 */
+		scx_bpf_kick_cpu(cpu, 0);
+		return;
+	}
+
+	/*
+	 * Kick the CPU if there are still tasks dispatched to the
+	 * corresponding per-CPU DSQ.
+	 */
+	if (scx_bpf_dsq_nr_queued(cpu_to_dsq(cpu)) > 0)
+		scx_bpf_kick_cpu(cpu, 0);
+}
+
+/*
+ * Task @p changes cpumask: update its local cpumask generation counter.
+ */
+void BPF_STRUCT_OPS(rustland_set_cpumask, struct task_struct *p,
+		    const struct cpumask *cpumask)
+{
+	struct task_ctx *tctx;
+
+	tctx = try_lookup_task_ctx(p);
+	if (!tctx)
+		return;
+	tctx->cpumask_cnt++;
+}
+
+/*
+ * A CPU is taken away from the scheduler, preempting the current task by
+ * another one running in a higher priority sched_class.
+ */
+void BPF_STRUCT_OPS(rustland_cpu_release, s32 cpu,
+				struct scx_cpu_release_args *args)
+{
+	struct task_struct *p = args->task;
+	/*
+	 * If the interrupted task is the user-space scheduler make sure to
+	 * re-schedule it immediately.
+	 */
+	dbg_msg("cpu preemption: pid=%d (%s)", p->pid, p->comm);
+	if (is_usersched_task(p))
+		set_usersched_needed();
+}
+
+void BPF_STRUCT_OPS(rustland_cpu_online, s32 cpu)
+{
+	/* Set the CPU state to online */
+	set_cpu_state(offline_cpumask, cpu, false);
+
+	__sync_fetch_and_add(&nr_online_cpus, 1);
+	__sync_fetch_and_add(&cpu_hotplug_cnt, 1);
+}
+
+void BPF_STRUCT_OPS(rustland_cpu_offline, s32 cpu)
+{
+	/* Set the CPU state to offline */
+	set_cpu_state(offline_cpumask, cpu, true);
+
+	__sync_fetch_and_sub(&nr_online_cpus, 1);
+	__sync_fetch_and_add(&cpu_hotplug_cnt, 1);
+
+	set_offline_needed();
+}
+
+/*
+ * A new task @p is being created.
+ *
+ * Allocate and initialize all the internal structures for the task (this
+ * function is allowed to block, so it can be used to preallocate memory).
+ */
+s32 BPF_STRUCT_OPS(rustland_init_task, struct task_struct *p,
+		   struct scx_init_task_args *args)
+{
+	struct task_ctx *tctx;
+	struct bpf_cpumask *cpumask;
+
+	tctx = bpf_task_storage_get(&task_ctx_stor, p, 0,
+				    BPF_LOCAL_STORAGE_GET_F_CREATE);
+	if (!tctx)
+		return -ENOMEM;
+	tctx->slice_ns = SCX_SLICE_DFL;
+
+	/*
+	 * Create task's L2 cache cpumask.
+	 */
+	cpumask = bpf_cpumask_create();
+	if (!cpumask)
+		return -ENOMEM;
+	cpumask = bpf_kptr_xchg(&tctx->l2_cpumask, cpumask);
+	if (cpumask)
+		bpf_cpumask_release(cpumask);
+
+	/*
+	 * Create task's L3 cache cpumask.
+	 */
+	cpumask = bpf_cpumask_create();
+	if (!cpumask)
+		return -ENOMEM;
+	cpumask = bpf_kptr_xchg(&tctx->l3_cpumask, cpumask);
+	if (cpumask)
+		bpf_cpumask_release(cpumask);
+
+	return 0;
+}
+
+/*
+ * Heartbeat scheduler timer callback.
+ *
+ * If the system is completely idle the sched-ext watchdog may incorrectly
+ * detect that as a stall and automatically disable the scheduler. So, use this
+ * timer to periodically wake-up the scheduler and avoid long inactivity.
+ *
+ * This can also help to prevent real "stalling" conditions in the scheduler.
+ */
+static int usersched_timer_fn(void *map, int *key, struct bpf_timer *timer)
+{
+	int err = 0;
+
+	/* Kick the scheduler */
+	set_usersched_needed();
+
+	/* Re-arm the timer */
+	err = bpf_timer_start(timer, USERSCHED_TIMER_NS, 0);
+	if (err)
+		scx_bpf_error("Failed to arm stats timer");
+
+	return 0;
+}
+
+/*
+ * Initialize the heartbeat scheduler timer.
+ */
+static int usersched_timer_init(void)
+{
+	struct bpf_timer *timer;
+	u32 key = 0;
+	int err;
+
+	timer = bpf_map_lookup_elem(&usersched_timer, &key);
+	if (!timer) {
+		scx_bpf_error("Failed to lookup scheduler timer");
+		return -ESRCH;
+	}
+	bpf_timer_init(timer, &usersched_timer, CLOCK_BOOTTIME);
+	bpf_timer_set_callback(timer, usersched_timer_fn);
+	err = bpf_timer_start(timer, USERSCHED_TIMER_NS, 0);
+	if (err)
+		scx_bpf_error("Failed to arm scheduler timer");
+
+	return err;
+}
+
+/*
+ * Evaluate the amount of online CPUs.
+ */
+s32 get_nr_online_cpus(void)
+{
+	const struct cpumask *online_cpumask;
+	u64 nr_cpu_ids = scx_bpf_nr_cpu_ids();
+	int i, cpus = 0;
+
+	online_cpumask = scx_bpf_get_online_cpumask();
+
+	bpf_for(i, 0, nr_cpu_ids) {
+		if (!bpf_cpumask_test_cpu(i, online_cpumask))
+			continue;
+		cpus++;
+	}
+
+	scx_bpf_put_cpumask(online_cpumask);
+
+	return cpus;
+}
+
+/*
+ * Create a DSQ for each CPU available in the system and a global shared DSQ.
+ *
+ * All the tasks processed by the user-space scheduler can be dispatched either
+ * to a specific CPU/DSQ or to the first CPU available (SHARED_DSQ).
+ *
+ * Custom DSQs are then consumed from the .dispatch() callback, that will
+ * transfer all the enqueued tasks to the consuming CPU's local DSQ.
+ */
+static int dsq_init(void)
+{
+	u64 nr_cpu_ids = scx_bpf_nr_cpu_ids();
+	int err;
+	s32 cpu;
+
+	/* Initialize amount of online CPUs */
+	nr_online_cpus = get_nr_online_cpus();
+
+	/* Create per-CPU DSQs */
+	bpf_for(cpu, 0, nr_cpu_ids) {
+		err = scx_bpf_create_dsq(cpu_to_dsq(cpu), -1);
+		if (err) {
+			scx_bpf_error("failed to create pcpu DSQ %d: %d",
+				      cpu, err);
+			return err;
+		}
+	}
+
+	/* Create the global shared DSQ */
+	err = scx_bpf_create_dsq(SHARED_DSQ, -1);
+	if (err) {
+		scx_bpf_error("failed to create shared DSQ: %d", err);
+		return err;
+	}
+
+	return 0;
+}
+
+static int init_cpumask(struct bpf_cpumask **cpumask)
+{
+	struct bpf_cpumask *mask;
+	int err = 0;
+
+	/*
+	 * Do nothing if the mask is already initialized.
+	 */
+	mask = *cpumask;
+	if (mask)
+		return 0;
+	/*
+	 * Create the CPU mask.
+	 */
+	err = calloc_cpumask(cpumask);
+	if (!err)
+		mask = *cpumask;
+	if (!mask)
+		err = -ENOMEM;
+
+	return err;
+}
+
+SEC("syscall")
+int enable_sibling_cpu(struct domain_arg *input)
+{
+	struct cpu_ctx *cctx;
+	struct bpf_cpumask *mask, **pmask;
+	int err = 0;
+
+	cctx = try_lookup_cpu_ctx(input->cpu_id);
+	if (!cctx)
+		return -ENOENT;
+
+	/* Make sure the target CPU mask is initialized */
+	switch (input->lvl_id) {
+	case 2:
+		pmask = &cctx->l2_cpumask;
+		break;
+	case 3:
+		pmask = &cctx->l3_cpumask;
+		break;
+	default:
+		return -EINVAL;
+	}
+	err = init_cpumask(pmask);
+	if (err)
+		return err;
+
+	bpf_rcu_read_lock();
+	mask = *pmask;
+	if (mask)
+		bpf_cpumask_set_cpu(input->sibling_cpu_id, mask);
+	bpf_rcu_read_unlock();
+
+	return err;
+}
+
+/*
+ * Initialize the scheduling class.
+ */
+s32 BPF_STRUCT_OPS_SLEEPABLE(rustland_init)
+{
+	struct bpf_cpumask *mask;
+	int err;
+
+	/* Compile-time checks */
+	BUILD_BUG_ON((MAX_CPUS % 2));
+
+	/* Initialize the offline CPU mask */
+	err = calloc_cpumask(&offline_cpumask);
+	mask = offline_cpumask;
+	if (!mask)
+		err = -ENOMEM;
+	if (err)
+		return err;
+
+	/* Initialize rustland core */
+	err = dsq_init();
+	if (err)
+		return err;
+	err = usersched_timer_init();
+	if (err)
+		return err;
+
+	return 0;
+}
+
+/*
+ * Unregister the scheduling class.
+ */
+void BPF_STRUCT_OPS(rustland_exit, struct scx_exit_info *ei)
+{
+	UEI_RECORD(uei, ei);
+}
+
+/*
+ * Scheduling class declaration.
+ */
+SCX_OPS_DEFINE(rustland,
+	       .select_cpu		= (void *)rustland_select_cpu,
+	       .enqueue			= (void *)rustland_enqueue,
+	       .dispatch		= (void *)rustland_dispatch,
+	       .running			= (void *)rustland_running,
+	       .stopping		= (void *)rustland_stopping,
+	       .update_idle		= (void *)rustland_update_idle,
+	       .set_cpumask		= (void *)rustland_set_cpumask,
+	       .cpu_release		= (void *)rustland_cpu_release,
+	       .cpu_online		= (void *)rustland_cpu_online,
+	       .cpu_offline		= (void *)rustland_cpu_offline,
+	       .init_task		= (void *)rustland_init_task,
+	       .init			= (void *)rustland_init,
+	       .exit			= (void *)rustland_exit,
+	       .flags			= SCX_OPS_ENQ_LAST | SCX_OPS_KEEP_BUILTIN_IDLE,
+	       .timeout_ms		= 5000,
+	       .dispatch_max_batch	= MAX_DISPATCH_SLOT,
+	       .name			= "rustland");
--- /dev/null
+++ scx-1.0.4.1/scheds/rust/scx_rlfifo/src/bpf.rs
@@ -0,0 +1,592 @@
+// Copyright (c) Andrea Righi <andrea.righi@linux.dev>
+
+// This software may be used and distributed according to the terms of the
+// GNU General Public License version 2.
+
+use std::mem::MaybeUninit;
+
+use crate::bpf_intf;
+use crate::bpf_intf::*;
+use crate::bpf_skel::*;
+
+use std::ffi::c_int;
+use std::ffi::c_ulong;
+use std::fs::File;
+use std::io::Read;
+
+use std::collections::HashMap;
+use std::sync::atomic::AtomicBool;
+use std::sync::atomic::Ordering;
+use std::sync::Arc;
+
+use anyhow::Context;
+use anyhow::Result;
+
+use plain::Plain;
+
+use libbpf_rs::skel::OpenSkel;
+use libbpf_rs::skel::Skel;
+use libbpf_rs::skel::SkelBuilder;
+use libbpf_rs::OpenObject;
+use libbpf_rs::ProgramInput;
+
+use libc::{pthread_self, pthread_setschedparam, sched_param};
+
+#[cfg(target_env = "musl")]
+use libc::timespec;
+
+use scx_utils::compat;
+use scx_utils::scx_ops_attach;
+use scx_utils::scx_ops_load;
+use scx_utils::scx_ops_open;
+use scx_utils::uei_exited;
+use scx_utils::uei_report;
+use scx_utils::Topology;
+use scx_utils::UserExitInfo;
+
+use scx_rustland_core::ALLOCATOR;
+
+// Defined in UAPI
+const SCHED_EXT: i32 = 7;
+
+// Allow to dispatch the task on any CPU.
+//
+// The task will be dispatched to the global shared DSQ and it will run on the first CPU available.
+#[allow(dead_code)]
+pub const RL_CPU_ANY: i32 = bpf_intf::RL_CPU_ANY as i32;
+
+/// High-level Rust abstraction to interact with a generic sched-ext BPF component.
+///
+/// Overview
+/// ========
+///
+/// The main BPF interface is provided by the BpfScheduler() struct. When this object is
+/// initialized it will take care of registering and initializing the BPF component.
+///
+/// The scheduler then can use BpfScheduler() instance to receive tasks (in the form of QueuedTask
+/// objects) and dispatch tasks (in the form of DispatchedTask objects), using respectively the
+/// methods dequeue_task() and dispatch_task().
+///
+/// BPF counters and statistics can be accessed using the methods nr_*_mut(), in particular
+/// nr_queued_mut() and nr_scheduled_mut() can be updated to notify the BPF component if the
+/// user-space scheduler has some pending work to do or not.
+///
+/// Finally the methods exited() and shutdown_and_report() can be used respectively to test
+/// whether the BPF component exited, and to shutdown and report the exit message.
+/// whether the BPF component exited, and to shutdown and report exit message.
+
+// Task queued for scheduling from the BPF component (see bpf_intf::queued_task_ctx).
+#[derive(Debug, PartialEq, Eq, PartialOrd, Clone)]
+pub struct QueuedTask {
+    pub pid: i32,              // pid that uniquely identifies a task
+    pub cpu: i32,              // CPU where the task is running
+    pub flags: u64,            // task enqueue flags
+    pub sum_exec_runtime: u64, // Total cpu time
+    pub weight: u64,           // Task static priority
+    cpumask_cnt: u64,          // cpumask generation counter (private)
+}
+
+// Task queued for dispatching to the BPF component (see bpf_intf::dispatched_task_ctx).
+#[derive(Debug, PartialEq, Eq, PartialOrd, Clone)]
+pub struct DispatchedTask {
+    pub pid: i32,      // pid that uniquely identifies a task
+    pub cpu: i32,      // target CPU selected by the scheduler
+    pub flags: u64,    // special dispatch flags
+    pub slice_ns: u64, // time slice assigned to the task (0 = default)
+    pub vtime: u64,    // task deadline / vruntime
+    cpumask_cnt: u64,  // cpumask generation counter (private)
+}
+
+impl DispatchedTask {
+    // Create a DispatchedTask from a QueuedTask.
+    //
+    // A dispatched task should be always originated from a QueuedTask (there is no reason to
+    // dispatch a task if it wasn't queued to the scheduler earlier).
+    pub fn new(task: &QueuedTask) -> Self {
+        DispatchedTask {
+            pid: task.pid,
+            cpu: task.cpu,
+            flags: task.flags,
+            cpumask_cnt: task.cpumask_cnt,
+            slice_ns: 0, // use default time slice
+            vtime: 0,
+        }
+    }
+}
+
+// Helpers used to submit tasks to the BPF user ring buffer.
+unsafe impl Plain for bpf_intf::dispatched_task_ctx {}
+
+impl AsMut<bpf_intf::dispatched_task_ctx> for bpf_intf::dispatched_task_ctx {
+    fn as_mut(&mut self) -> &mut bpf_intf::dispatched_task_ctx {
+        self
+    }
+}
+
+// Message received from the dispatcher (see bpf_intf::queued_task_ctx for details).
+//
+// NOTE: eventually libbpf-rs will provide a better abstraction for this.
+struct EnqueuedMessage {
+    inner: bpf_intf::queued_task_ctx,
+}
+
+impl EnqueuedMessage {
+    fn from_bytes(bytes: &[u8]) -> Self {
+        let queued_task_struct = unsafe { *(bytes.as_ptr() as *const bpf_intf::queued_task_ctx) };
+        EnqueuedMessage {
+            inner: queued_task_struct,
+        }
+    }
+
+    fn to_queued_task(&self) -> QueuedTask {
+        QueuedTask {
+            pid: self.inner.pid,
+            cpu: self.inner.cpu,
+            flags: self.inner.flags,
+            sum_exec_runtime: self.inner.sum_exec_runtime,
+            weight: self.inner.weight,
+            cpumask_cnt: self.inner.cpumask_cnt,
+        }
+    }
+}
+
+pub struct BpfScheduler<'cb> {
+    pub skel: BpfSkel<'cb>,                // Low-level BPF connector
+    shutdown: Arc<AtomicBool>,             // Determine scheduler shutdown
+    queued: libbpf_rs::RingBuffer<'cb>,    // Ring buffer of queued tasks
+    dispatched: libbpf_rs::UserRingBuffer, // User Ring buffer of dispatched tasks
+    cpu_hotplug_cnt: u64,                  // CPU hotplug generation counter
+    struct_ops: Option<libbpf_rs::Link>,   // Low-level BPF methods
+}
+
+// Buffer to store a task read from the ring buffer.
+//
+// NOTE: make the buffer aligned to 64-bits to prevent misaligned dereferences when accessing the
+// buffer using a pointer.
+const BUFSIZE: usize = std::mem::size_of::<QueuedTask>();
+
+#[repr(align(8))]
+struct AlignedBuffer([u8; BUFSIZE]);
+
+static mut BUF: AlignedBuffer = AlignedBuffer([0; BUFSIZE]);
+
+// Special negative error code for libbpf to stop after consuming just one item from a BPF
+// ring buffer.
+const LIBBPF_STOP: i32 = -255;
+
+fn is_smt_active() -> std::io::Result<bool> {
+    let mut file = File::open("/sys/devices/system/cpu/smt/active")?;
+    let mut contents = String::new();
+    file.read_to_string(&mut contents)?;
+
+    let smt_active: i32 = contents.trim().parse().unwrap_or(0);
+
+    Ok(smt_active == 1)
+}
+
+impl<'cb> BpfScheduler<'cb> {
+    pub fn init(
+        open_object: &'cb mut MaybeUninit<OpenObject>,
+        exit_dump_len: u32,
+        partial: bool,
+        debug: bool,
+    ) -> Result<Self> {
+        let shutdown = Arc::new(AtomicBool::new(false));
+        let shutdown_clone = shutdown.clone();
+        ctrlc::set_handler(move || {
+            shutdown_clone.store(true, Ordering::Relaxed);
+        })
+        .context("Error setting Ctrl-C handler")?;
+
+        // Open the BPF prog first for verification.
+        let mut skel_builder = BpfSkelBuilder::default();
+        skel_builder.obj_builder.debug(debug);
+        let mut skel = scx_ops_open!(skel_builder, open_object, rustland)?;
+
+        // Lock all the memory to prevent page faults that could trigger potential deadlocks during
+        // scheduling.
+        ALLOCATOR.lock_memory();
+
+        // Copy one item from the ring buffer.
+        //
+        // # Safety
+        //
+        // Each invocation of the callback will trigger the copy of exactly one QueuedTask item to
+        // BUF. The caller must be synchronize to ensure that multiple invocations of the callback
+        // are not happening at the same time, but this is implicitly guaranteed by the fact that
+        // the caller is a single-thread process (for now).
+        //
+        // Use of a `str` whose contents are not valid UTF-8 is undefined behavior.
+        fn callback(data: &[u8]) -> i32 {
+            unsafe {
+                // SAFETY: copying from the BPF ring buffer to BUF is safe, since the size of BUF
+                // is exactly the size of QueuedTask and the callback operates in chunks of
+                // QueuedTask items. It also copies exactly one QueuedTask at a time, this is
+                // guaranteed by the error code returned by this callback (see below). From a
+                // thread-safety perspective this is also correct, assuming the caller is a
+                // single-thread process (as it is for now).
+                BUF.0.copy_from_slice(data);
+            }
+
+            // Return an unsupported error to stop early and consume only one item.
+            //
+            // NOTE: this is quite a hack. I wish libbpf would honor stopping after the first item
+            // is consumed, upon returning a non-zero positive value here, but it doesn't seem to
+            // be the case:
+            //
+            // https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/tree/tools/lib/bpf/ringbuf.c?h=v6.8-rc5#n260
+            //
+            // Maybe we should fix this to stop processing items from the ring buffer also when a
+            // value > 0 is returned.
+            //
+            LIBBPF_STOP
+        }
+
+        // Check host topology to determine if we need to enable SMT capabilities.
+        skel.maps.rodata_data.smt_enabled = is_smt_active()?;
+
+        // Set scheduler options (defined in the BPF part).
+        if partial {
+            skel.struct_ops.rustland_mut().flags |= *compat::SCX_OPS_SWITCH_PARTIAL;
+        }
+        skel.struct_ops.rustland_mut().exit_dump_len = exit_dump_len;
+
+        skel.maps.bss_data.usersched_pid = std::process::id();
+        skel.maps.rodata_data.debug = debug;
+
+        // Attach BPF scheduler.
+        let mut skel = scx_ops_load!(skel, rustland, uei)?;
+
+        // Initialize cache domains.
+        let topo = Topology::new().unwrap();
+        Self::init_l2_cache_domains(&mut skel, &topo)?;
+        Self::init_l3_cache_domains(&mut skel, &topo)?;
+
+        let struct_ops = Some(scx_ops_attach!(skel, rustland)?);
+
+        // Build the ring buffer of queued tasks.
+        let maps = &skel.maps;
+        let queued_ring_buffer = &maps.queued;
+        let mut rbb = libbpf_rs::RingBufferBuilder::new();
+        rbb.add(queued_ring_buffer, callback)
+            .expect("failed to add ringbuf callback");
+        let queued = rbb.build().expect("failed to build ringbuf");
+
+        // Build the user ring buffer of dispatched tasks.
+        let dispatched = libbpf_rs::UserRingBuffer::new(&maps.dispatched)
+            .expect("failed to create user ringbuf");
+
+        // Make sure to use the SCHED_EXT class at least for the scheduler itself.
+        match Self::use_sched_ext() {
+            0 => Ok(Self {
+                skel,
+                shutdown,
+                queued,
+                dispatched,
+                cpu_hotplug_cnt: 0,
+                struct_ops,
+            }),
+            err => Err(anyhow::Error::msg(format!(
+                "sched_setscheduler error: {}",
+                err
+            ))),
+        }
+    }
+
+    fn enable_sibling_cpu(
+        skel: &mut BpfSkel<'_>,
+        lvl: usize,
+        cpu: usize,
+        sibling_cpu: usize,
+    ) -> Result<(), u32> {
+        let prog = &mut skel.progs.enable_sibling_cpu;
+        let mut args = domain_arg {
+            lvl_id: lvl as c_int,
+            cpu_id: cpu as c_int,
+            sibling_cpu_id: sibling_cpu as c_int,
+        };
+        let input = ProgramInput {
+            context_in: Some(unsafe {
+                std::slice::from_raw_parts_mut(
+                    &mut args as *mut _ as *mut u8,
+                    std::mem::size_of_val(&args),
+                )
+            }),
+            ..Default::default()
+        };
+        let out = prog.test_run(input).unwrap();
+        if out.return_value != 0 {
+            return Err(out.return_value);
+        }
+
+        Ok(())
+    }
+
+    fn init_cache_domains(
+        skel: &mut BpfSkel<'_>,
+        topo: &Topology,
+        cache_lvl: usize,
+        enable_sibling_cpu_fn: &dyn Fn(&mut BpfSkel<'_>, usize, usize, usize) -> Result<(), u32>,
+    ) -> Result<(), std::io::Error> {
+        // Determine the list of CPU IDs associated to each cache node.
+        let mut cache_id_map: HashMap<usize, Vec<usize>> = HashMap::new();
+        for core in topo.cores().into_iter() {
+            for (cpu_id, cpu) in core.cpus() {
+                let cache_id = match cache_lvl {
+                    2 => cpu.l2_id(),
+                    3 => cpu.l3_id(),
+                    _ => panic!("invalid cache level {}", cache_lvl),
+                };
+                cache_id_map
+                    .entry(cache_id)
+                    .or_insert_with(Vec::new)
+                    .push(*cpu_id);
+            }
+        }
+
+        // Update the BPF cpumasks for the cache domains.
+        for (_cache_id, cpus) in cache_id_map {
+            for cpu in &cpus {
+                for sibling_cpu in &cpus {
+                    match enable_sibling_cpu_fn(skel, cache_lvl, *cpu, *sibling_cpu) {
+                        Ok(()) => {}
+                        Err(_) => {}
+                    }
+                }
+            }
+        }
+
+        Ok(())
+    }
+
+    fn init_l2_cache_domains(
+        skel: &mut BpfSkel<'_>,
+        topo: &Topology,
+    ) -> Result<(), std::io::Error> {
+        Self::init_cache_domains(skel, topo, 2, &|skel, lvl, cpu, sibling_cpu| {
+            Self::enable_sibling_cpu(skel, lvl, cpu, sibling_cpu)
+        })
+    }
+
+    fn init_l3_cache_domains(
+        skel: &mut BpfSkel<'_>,
+        topo: &Topology,
+    ) -> Result<(), std::io::Error> {
+        Self::init_cache_domains(skel, topo, 3, &|skel, lvl, cpu, sibling_cpu| {
+            Self::enable_sibling_cpu(skel, lvl, cpu, sibling_cpu)
+        })
+    }
+
+    fn refresh_cache_domains(&mut self) {
+        // Check if we need to refresh the CPU cache information.
+        if self.cpu_hotplug_cnt == self.skel.maps.bss_data.cpu_hotplug_cnt {
+            return;
+        }
+
+        // Re-initialize cache domains.
+        let topo = Topology::new().unwrap();
+        Self::init_l2_cache_domains(&mut self.skel, &topo).unwrap();
+        Self::init_l3_cache_domains(&mut self.skel, &topo).unwrap();
+
+        // Update CPU hotplug generation counter.
+        self.cpu_hotplug_cnt = self.skel.maps.bss_data.cpu_hotplug_cnt;
+    }
+
+    // Notify the BPF component that the user-space scheduler has completed its scheduling cycle,
+    // updating the amount tasks that are still peding.
+    //
+    // NOTE: do not set allow(dead_code) for this method, any scheduler must use this method at
+    // some point, otherwise the BPF component will keep waking-up the user-space scheduler in a
+    // busy loop, causing unnecessary high CPU consumption.
+    pub fn notify_complete(&mut self, nr_pending: u64) {
+        self.refresh_cache_domains();
+        self.skel.maps.bss_data.nr_scheduled = nr_pending;
+        std::thread::yield_now();
+    }
+
+    // Counter of the online CPUs.
+    #[allow(dead_code)]
+    pub fn nr_online_cpus_mut(&mut self) -> &mut u64 {
+        &mut self.skel.maps.bss_data.nr_online_cpus
+    }
+
+    // Counter of currently running tasks.
+    #[allow(dead_code)]
+    pub fn nr_running_mut(&mut self) -> &mut u64 {
+        &mut self.skel.maps.bss_data.nr_running
+    }
+
+    // Counter of queued tasks.
+    #[allow(dead_code)]
+    pub fn nr_queued_mut(&mut self) -> &mut u64 {
+        &mut self.skel.maps.bss_data.nr_queued
+    }
+
+    // Counter of scheduled tasks.
+    #[allow(dead_code)]
+    pub fn nr_scheduled_mut(&mut self) -> &mut u64 {
+        &mut self.skel.maps.bss_data.nr_scheduled
+    }
+
+    // Counter of user dispatch events.
+    #[allow(dead_code)]
+    pub fn nr_user_dispatches_mut(&mut self) -> &mut u64 {
+        &mut self.skel.maps.bss_data.nr_user_dispatches
+    }
+
+    // Counter of user kernel events.
+    #[allow(dead_code)]
+    pub fn nr_kernel_dispatches_mut(&mut self) -> &mut u64 {
+        &mut self.skel.maps.bss_data.nr_kernel_dispatches
+    }
+
+    // Counter of cancel dispatch events.
+    #[allow(dead_code)]
+    pub fn nr_cancel_dispatches_mut(&mut self) -> &mut u64 {
+        &mut self.skel.maps.bss_data.nr_cancel_dispatches
+    }
+
+    // Counter of dispatches bounced to the shared DSQ.
+    #[allow(dead_code)]
+    pub fn nr_bounce_dispatches_mut(&mut self) -> &mut u64 {
+        &mut self.skel.maps.bss_data.nr_bounce_dispatches
+    }
+
+    // Counter of failed dispatch events.
+    #[allow(dead_code)]
+    pub fn nr_failed_dispatches_mut(&mut self) -> &mut u64 {
+        &mut self.skel.maps.bss_data.nr_failed_dispatches
+    }
+
+    // Counter of scheduler congestion events.
+    #[allow(dead_code)]
+    pub fn nr_sched_congested_mut(&mut self) -> &mut u64 {
+        &mut self.skel.maps.bss_data.nr_sched_congested
+    }
+
+    // Set scheduling class for the scheduler itself to SCHED_EXT
+    fn use_sched_ext() -> i32 {
+        #[cfg(target_env = "gnu")]
+        let param: sched_param = sched_param { sched_priority: 0 };
+        #[cfg(target_env = "musl")]
+        let param: sched_param = sched_param {
+            sched_priority: 0,
+            sched_ss_low_priority: 0,
+            sched_ss_repl_period: timespec {
+                tv_sec: 0,
+                tv_nsec: 0,
+            },
+            sched_ss_init_budget: timespec {
+                tv_sec: 0,
+                tv_nsec: 0,
+            },
+            sched_ss_max_repl: 0,
+        };
+
+        unsafe { pthread_setschedparam(pthread_self(), SCHED_EXT, &param as *const sched_param) }
+    }
+
+    // Pick an idle CPU for the target PID.
+    pub fn select_cpu(&mut self, pid: i32, cpu: i32, flags: u64) -> i32 {
+        let prog = &mut self.skel.progs.rs_select_cpu;
+        let mut args = task_cpu_arg {
+            pid: pid as c_int,
+            cpu: cpu as c_int,
+            flags: flags as c_ulong,
+        };
+        let input = ProgramInput {
+            context_in: Some(unsafe {
+                std::slice::from_raw_parts_mut(
+                    &mut args as *mut _ as *mut u8,
+                    std::mem::size_of_val(&args),
+                )
+            }),
+            ..Default::default()
+        };
+        let out = prog.test_run(input).unwrap();
+
+        out.return_value as i32
+    }
+
+    // Receive a task to be scheduled from the BPF dispatcher.
+    pub fn dequeue_task(&mut self) -> Result<Option<QueuedTask>, i32> {
+        match self.queued.consume_raw() {
+            0 => {
+                self.skel.maps.bss_data.nr_queued = 0;
+                Ok(None)
+            }
+            LIBBPF_STOP => {
+                // A valid task is received, convert data to a proper task struct.
+                let task = unsafe { EnqueuedMessage::from_bytes(&BUF.0).to_queued_task() };
+                let _ = self.skel.maps.bss_data.nr_queued.saturating_sub(1);
+
+                Ok(Some(task))
+            }
+            res if res < 0 => Err(res),
+            res => panic!(
+                "Unexpected return value from libbpf-rs::consume_raw(): {}",
+                res
+            ),
+        }
+    }
+
+    // Send a task to the dispatcher.
+    pub fn dispatch_task(&mut self, task: &DispatchedTask) -> Result<(), libbpf_rs::Error> {
+        // Reserve a slot in the user ring buffer.
+        let mut urb_sample = self
+            .dispatched
+            .reserve(std::mem::size_of::<bpf_intf::dispatched_task_ctx>())?;
+        let bytes = urb_sample.as_mut();
+        let dispatched_task = plain::from_mut_bytes::<bpf_intf::dispatched_task_ctx>(bytes)
+            .expect("failed to convert bytes");
+
+        // Convert the dispatched task into the low-level dispatched task context.
+        let bpf_intf::dispatched_task_ctx {
+            pid,
+            cpu,
+            flags,
+            slice_ns,
+            vtime,
+            cpumask_cnt,
+            ..
+        } = &mut dispatched_task.as_mut();
+
+        *pid = task.pid;
+        *cpu = task.cpu;
+        *flags = task.flags;
+        *slice_ns = task.slice_ns;
+        *vtime = task.vtime;
+        *cpumask_cnt = task.cpumask_cnt;
+
+        // Store the task in the user ring buffer.
+        //
+        // NOTE: submit() only updates the reserved slot in the user ring buffer, so it is not
+        // expected to fail.
+        self.dispatched
+            .submit(urb_sample)
+            .expect("failed to submit task");
+
+        Ok(())
+    }
+
+    // Read exit code from the BPF part.
+    pub fn exited(&mut self) -> bool {
+        self.shutdown.load(Ordering::Relaxed) || uei_exited!(&self.skel, uei)
+    }
+
+    // Called on exit to shutdown and report exit message from the BPF part.
+    pub fn shutdown_and_report(&mut self) -> Result<UserExitInfo> {
+        self.struct_ops.take();
+        uei_report!(&self.skel, uei)
+    }
+}
+
+// Disconnect the low-level BPF scheduler.
+impl<'a> Drop for BpfScheduler<'a> {
+    fn drop(&mut self) {
+        if let Some(struct_ops) = self.struct_ops.take() {
+            drop(struct_ops);
+        }
+        ALLOCATOR.unlock_memory();
+    }
+}
--- /dev/null
+++ scx-1.0.4.1/scheds/rust/scx_rustland/intf.h
@@ -0,0 +1,106 @@
+// This software may be used and distributed according to the terms of the
+// GNU General Public License version 2.
+
+#ifndef __INTF_H
+#define __INTF_H
+
+#define MAX(x, y) ((x) > (y) ? (x) : (y))
+#define MIN(x, y) ((x) < (y) ? (x) : (y))
+
+#define NSEC_PER_SEC	1000000000L
+#define CLOCK_BOOTTIME	7
+
+#include <stdbool.h>
+#ifndef __kptr
+#ifdef __KERNEL__
+#error "__kptr_ref not defined in the kernel"
+#endif
+#define __kptr
+#endif
+
+#ifndef __VMLINUX_H__
+typedef unsigned char u8;
+typedef unsigned short u16;
+typedef unsigned int u32;
+typedef unsigned long u64;
+
+typedef signed char s8;
+typedef signed short s16;
+typedef signed int s32;
+typedef signed long s64;
+
+typedef int pid_t;
+#endif /* __VMLINUX_H__ */
+
+/* Check a condition at build time */
+#define BUILD_BUG_ON(expr) \
+	do { \
+		extern char __build_assert__[(expr) ? -1 : 1] \
+			__attribute__((unused)); \
+	} while(0)
+
+/*
+ * Maximum amount of CPUs supported by this scheduler (this defines the size of
+ * cpu_map that is used to store the idle state and CPU ownership).
+ */
+#define MAX_CPUS 1024
+
+/* Special dispatch flags */
+enum {
+	/*
+	 * Do not assign any specific CPU to the task.
+	 *
+	 * The task will be dispatched to the global shared DSQ and it will run
+	 * on the first CPU available.
+	 */
+	RL_CPU_ANY = 1 << 20,
+};
+
+/*
+ * Specify a target CPU for a specific PID.
+ */
+struct task_cpu_arg {
+	pid_t pid;
+	s32 cpu;
+	u64 flags;
+};
+
+/*
+ * Specify a sibling CPU relationship for a specific scheduling domain.
+ */
+struct domain_arg {
+	s32 lvl_id;
+	s32 cpu_id;
+	s32 sibling_cpu_id;
+};
+
+/*
+ * Task sent to the user-space scheduler by the BPF dispatcher.
+ *
+ * All attributes are collected from the kernel by the the BPF component.
+ */
+struct queued_task_ctx {
+	s32 pid;
+	s32 cpu; /* CPU where the task is running */
+	u64 flags; /* task enqueue flags */
+	u64 cpumask_cnt; /* cpumask generation counter */
+	u64 sum_exec_runtime; /* Total cpu time */
+	u64 weight; /* Task static priority */
+};
+
+/*
+ * Task sent to the BPF dispatcher by the user-space scheduler.
+ *
+ * This struct can be easily extended to send more information to the
+ * dispatcher (i.e., a target CPU, a variable time slice, etc.).
+ */
+struct dispatched_task_ctx {
+	s32 pid;
+	s32 cpu; /* CPU where the task should be dispatched */
+	u64 flags; /* task enqueue flags */
+	u64 slice_ns; /* time slice assigned to the task (0=default) */
+	u64 vtime; /* task deadline / vruntime */
+	u64 cpumask_cnt; /* cpumask generation counter */
+};
+
+#endif /* __INTF_H */
--- /dev/null
+++ scx-1.0.4.1/scheds/rust/scx_rustland/main.bpf.c
@@ -0,0 +1,1418 @@
+/* Copyright (c) Andrea Righi <andrea.righi@linux.dev> */
+/*
+ * scx_rustland_core: BPF backend for schedulers running in user-space.
+ *
+ * This BPF backend implements the low level sched-ext functionalities for a
+ * user-space counterpart, that implements the actual scheduling policy.
+ *
+ * The BPF part collects total cputime and weight from the tasks that need to
+ * run, then it sends all details to the user-space scheduler that decides the
+ * best order of execution of the tasks (based on the collected metrics).
+ *
+ * The user-space scheduler then returns to the BPF component the list of tasks
+ * to be dispatched in the proper order.
+ *
+ * Messages between the BPF component and the user-space scheduler are passed
+ * using BPF_MAP_TYPE_RINGBUFFER / BPF_MAP_TYPE_USER_RINGBUF maps: @queued for
+ * the messages sent by the BPF dispatcher to the user-space scheduler and
+ * @dispatched for the messages sent by the user-space scheduler to the BPF
+ * dispatcher.
+ *
+ * The BPF dispatcher is completely agnostic of the particular scheduling
+ * policy implemented in user-space. For this reason developers that are
+ * willing to use this scheduler to experiment scheduling policies should be
+ * able to simply modify the Rust component, without having to deal with any
+ * internal kernel / BPF details.
+ *
+ * This software may be used and distributed according to the terms of the
+ * GNU General Public License version 2.
+ */
+#include <scx/common.bpf.h>
+#include "intf.h"
+
+char _license[] SEC("license") = "GPL";
+
+UEI_DEFINE(uei);
+
+/*
+ * Introduce a custom DSQ shared across all the CPUs, where we can dispatch
+ * tasks that will be executed on the first CPU available.
+ *
+ * Per-CPU DSQs are also provided, to allow the scheduler to run a task on a
+ * specific CPU (see dsq_init()).
+ */
+#define SHARED_DSQ MAX_CPUS
+
+/*
+ * Scheduler attributes and statistics.
+ */
+u32 usersched_pid; /* User-space scheduler PID */
+const volatile bool switch_partial; /* Switch all tasks or SCHED_EXT tasks */
+
+/*
+ * Number of tasks that are queued for scheduling.
+ *
+ * This number is incremented by the BPF component when a task is queued to the
+ * user-space scheduler and it must be decremented by the user-space scheduler
+ * when a task is consumed.
+ */
+volatile u64 nr_queued;
+
+/*
+ * Number of tasks that are waiting for scheduling.
+ *
+ * This number must be updated by the user-space scheduler to keep track if
+ * there is still some scheduling work to do.
+ */
+volatile u64 nr_scheduled;
+
+/*
+ * Amount of currently running tasks.
+ */
+volatile u64 nr_running, nr_online_cpus;
+
+/* Dispatch statistics */
+volatile u64 nr_user_dispatches, nr_kernel_dispatches,
+	     nr_cancel_dispatches, nr_bounce_dispatches;
+
+/* Failure statistics */
+volatile u64 nr_failed_dispatches, nr_sched_congested;
+
+ /* Report additional debugging information */
+const volatile bool debug;
+
+/* Allow to use bpf_printk() only when @debug is set */
+#define dbg_msg(_fmt, ...) do {						\
+	if (debug)							\
+		bpf_printk(_fmt, ##__VA_ARGS__);			\
+} while(0)
+
+/*
+ * CPUs in the system have SMT is enabled.
+ */
+const volatile bool smt_enabled = true;
+
+/*
+ * Mask of offline CPUs, used to properly support CPU hotplugging.
+ */
+private(BPFLAND) struct bpf_cpumask __kptr *offline_cpumask;
+
+/*
+ * CPU hotplugging generation counter (used to notify the user-space
+ * counterpart when a CPU hotplug event happened, allowing it to refresh the
+ * topology information).
+ */
+volatile u64 cpu_hotplug_cnt;
+
+/*
+ * Set the state of a CPU in a cpumask.
+ */
+static bool set_cpu_state(struct bpf_cpumask *cpumask, s32 cpu, bool state)
+{
+	if (!cpumask)
+		return false;
+	if (state)
+		return bpf_cpumask_test_and_set_cpu(cpu, cpumask);
+	else
+		return bpf_cpumask_test_and_clear_cpu(cpu, cpumask);
+}
+
+/*
+ * Access a cpumask in read-only mode (typically to check bits).
+ */
+static const struct cpumask *cast_mask(struct bpf_cpumask *mask)
+{
+	return (const struct cpumask *)mask;
+}
+
+/*
+ * Allocate/re-allocate a new cpumask.
+ */
+static int calloc_cpumask(struct bpf_cpumask **p_cpumask)
+{
+	struct bpf_cpumask *cpumask;
+
+	cpumask = bpf_cpumask_create();
+	if (!cpumask)
+		return -ENOMEM;
+
+	cpumask = bpf_kptr_xchg(p_cpumask, cpumask);
+	if (cpumask)
+		bpf_cpumask_release(cpumask);
+
+	return 0;
+}
+
+/*
+ * Determine when we need to drain tasks dispatched to CPUs that went offline.
+ */
+static int offline_needed;
+
+/*
+ * Notify the scheduler that we need to drain and re-enqueue the tasks
+ * dispatched to the offline CPU DSQs.
+ */
+static void set_offline_needed(void)
+{
+	__sync_fetch_and_or(&offline_needed, 1);
+}
+
+/*
+ * Check and clear the state of the offline CPUs re-enqueuing.
+ */
+static bool test_and_clear_offline_needed(void)
+{
+	return __sync_fetch_and_and(&offline_needed, 0) == 1;
+}
+
+/*
+ * Maximum amount of tasks queued between kernel and user-space at a certain
+ * time.
+ *
+ * The @queued and @dispatched lists are used in a producer/consumer fashion
+ * between the BPF part and the user-space part.
+ */
+#define MAX_ENQUEUED_TASKS 4096
+
+/*
+ * Maximum amount of slots reserved to the tasks dispatched via shared queue.
+ */
+#define MAX_DISPATCH_SLOT (MAX_ENQUEUED_TASKS / 8)
+
+/*
+ * The map containing tasks that are queued to user space from the kernel.
+ *
+ * This map is drained by the user space scheduler.
+ */
+struct {
+	__uint(type, BPF_MAP_TYPE_RINGBUF);
+	__uint(max_entries, MAX_ENQUEUED_TASKS *
+				sizeof(struct queued_task_ctx));
+} queued SEC(".maps");
+
+/*
+ * The user ring buffer containing pids that are dispatched from user space to
+ * the kernel.
+ *
+ * Drained by the kernel in .dispatch().
+ */
+struct {
+        __uint(type, BPF_MAP_TYPE_USER_RINGBUF);
+	__uint(max_entries, MAX_ENQUEUED_TASKS *
+				sizeof(struct dispatched_task_ctx));
+} dispatched SEC(".maps");
+
+/*
+ * Per-CPU context.
+ */
+struct cpu_ctx {
+	struct bpf_cpumask __kptr *l2_cpumask;
+	struct bpf_cpumask __kptr *l3_cpumask;
+};
+
+struct {
+	__uint(type, BPF_MAP_TYPE_PERCPU_ARRAY);
+	__type(key, u32);
+	__type(value, struct cpu_ctx);
+	__uint(max_entries, 1);
+} cpu_ctx_stor SEC(".maps");
+
+/*
+ * Return a CPU context.
+ */
+struct cpu_ctx *try_lookup_cpu_ctx(s32 cpu)
+{
+	const u32 idx = 0;
+	return bpf_map_lookup_percpu_elem(&cpu_ctx_stor, &idx, cpu);
+}
+
+/*
+ * Per-task local storage.
+ *
+ * This contain all the per-task information used internally by the BPF code.
+ */
+struct task_ctx {
+	/*
+	 * Temporary cpumask for calculating scheduling domains.
+	 */
+	struct bpf_cpumask __kptr *l2_cpumask;
+	struct bpf_cpumask __kptr *l3_cpumask;
+
+	/*
+	 * Time slice assigned to the task.
+	 */
+	u64 slice_ns;
+
+	/*
+	 * cpumask generation counter: used to verify the validity of the
+	 * current task's cpumask.
+	 */
+	u64 cpumask_cnt;
+};
+
+/* Map that contains task-local storage. */
+struct {
+	__uint(type, BPF_MAP_TYPE_TASK_STORAGE);
+	__uint(map_flags, BPF_F_NO_PREALLOC);
+	__type(key, int);
+	__type(value, struct task_ctx);
+} task_ctx_stor SEC(".maps");
+
+/*
+ * Return a local task context from a generic task or NULL if the context
+ * doesn't exist.
+ */
+struct task_ctx *try_lookup_task_ctx(const struct task_struct *p)
+{
+	struct task_ctx *tctx = bpf_task_storage_get(&task_ctx_stor,
+						(struct task_struct *)p, 0, 0);
+	if (!tctx)
+		dbg_msg("warning: failed to get task context for pid=%d (%s)",
+			p->pid, p->comm);
+	return tctx;
+}
+
+/*
+ * Intercept when a task is executing sched_setaffinity().
+ */
+struct {
+	__uint(type, BPF_MAP_TYPE_HASH);
+	__type(key, __u32);
+	__type(value, __u64);
+	__uint(max_entries, MAX_ENQUEUED_TASKS);
+} pid_setaffinity_map SEC(".maps");
+
+SEC("kprobe/sched_setaffinity")
+int BPF_KPROBE(kprobe_sched_setaffinity, struct task_struct *task,
+			const struct cpumask *new_mask)
+{
+	pid_t pid = bpf_get_current_pid_tgid() >> 32;
+	u64 value = true;
+
+	bpf_map_update_elem(&pid_setaffinity_map, &pid, &value, BPF_ANY);
+
+	return 0;
+}
+
+SEC("kretprobe/sched_setaffinity")
+int BPF_KRETPROBE(kretprobe_sched_setaffinity)
+{
+	pid_t pid = bpf_get_current_pid_tgid() >> 32;
+	bpf_map_delete_elem(&pid_setaffinity_map, &pid);
+
+	return 0;
+}
+
+/*
+ * Return true if a task is executing sched_setaffinity(), false otherwise.
+ */
+static bool in_setaffinity(pid_t pid)
+{
+	u64 *value = bpf_map_lookup_elem(&pid_setaffinity_map, &pid);
+	return value != NULL;
+}
+
+/*
+ * Heartbeat timer used to periodically trigger the check to run the user-space
+ * scheduler.
+ *
+ * Without this timer we may starve the scheduler if the system is completely
+ * idle and hit the watchdog that would auto-kill this scheduler.
+ */
+struct usersched_timer {
+	struct bpf_timer timer;
+};
+
+struct {
+	__uint(type, BPF_MAP_TYPE_ARRAY);
+	__uint(max_entries, 1);
+	__type(key, u32);
+	__type(value, struct usersched_timer);
+} usersched_timer SEC(".maps");
+
+/*
+ * Time period of the scheduler heartbeat, used to periodically kick the
+ * user-space scheduler and check if there is any pending activity.
+ */
+#define USERSCHED_TIMER_NS (NSEC_PER_SEC / 10)
+
+/*
+ * Return true if the target task @p is the user-space scheduler.
+ */
+static inline bool is_usersched_task(const struct task_struct *p)
+{
+	return p->pid == usersched_pid;
+}
+
+/*
+ * Return true if the target task @p is a kernel thread.
+ */
+static inline bool is_kthread(const struct task_struct *p)
+{
+	return p->flags & PF_KTHREAD;
+}
+
+/*
+ * Flag used to wake-up the user-space scheduler.
+ */
+static volatile u32 usersched_needed;
+
+/*
+ * Set user-space scheduler wake-up flag (equivalent to an atomic release
+ * operation).
+ */
+static void set_usersched_needed(void)
+{
+	__sync_fetch_and_or(&usersched_needed, 1);
+}
+
+/*
+ * Check and clear user-space scheduler wake-up flag (equivalent to an atomic
+ * acquire operation).
+ */
+static bool test_and_clear_usersched_needed(void)
+{
+	return __sync_fetch_and_and(&usersched_needed, 0) == 1;
+}
+
+/*
+ * Return true if there's any pending activity to do for the scheduler, false
+ * otherwise.
+ *
+ * NOTE: nr_queued is incremented by the BPF component, more exactly in
+ * enqueue(), when a task is sent to the user-space scheduler, then the
+ * scheduler drains the queued tasks (updating nr_queued) and adds them to its
+ * internal data structures / state; at this point tasks become "scheduled" and
+ * the user-space scheduler will take care of updating nr_scheduled
+ * accordingly; lastly tasks will be dispatched and the user-space scheduler
+ * will update nr_scheduled again.
+ *
+ * Checking both counters allows to determine if there is still some pending
+ * work to do for the scheduler: new tasks have been queued since last check,
+ * or there are still tasks "queued" or "scheduled" since the previous
+ * user-space scheduler run. If the counters are both zero it is pointless to
+ * wake-up the scheduler (even if a CPU becomes idle), because there is nothing
+ * to do.
+ *
+ * Also keep in mind that we don't need any protection here since this code
+ * doesn't run concurrently with the user-space scheduler (that is single
+ * threaded), therefore this check is also safe from a concurrency perspective.
+ */
+static bool usersched_has_pending_tasks(void)
+{
+	return nr_queued || nr_scheduled;
+}
+
+/*
+ * Return the corresponding CPU associated to a DSQ.
+ */
+static s32 dsq_to_cpu(u64 dsq_id)
+{
+	if (dsq_id >= MAX_CPUS) {
+		scx_bpf_error("Invalid dsq_id: %llu", dsq_id);
+		return -EINVAL;
+	}
+	return (s32)dsq_id;
+}
+
+/*
+ * Return the DSQ ID associated to a CPU, or SHARED_DSQ if the CPU is not
+ * valid.
+ */
+static u64 cpu_to_dsq(s32 cpu)
+{
+	if (cpu < 0 || cpu >= MAX_CPUS) {
+		scx_bpf_error("Invalid cpu: %d", cpu);
+		return SHARED_DSQ;
+	}
+	return (u64)cpu;
+}
+
+/*
+ * Return the time slice assigned to the task.
+ */
+static inline u64 task_slice(struct task_struct *p)
+{
+	struct task_ctx *tctx;
+
+	tctx = try_lookup_task_ctx(p);
+	if (!tctx || !tctx->slice_ns)
+		return SCX_SLICE_DFL;
+	return tctx->slice_ns;
+}
+
+/*
+ * Find an idle CPU in the system for the task.
+ *
+ * NOTE: the idle CPU selection doesn't need to be formally perfect, it is
+ * totally fine to accept racy conditions and potentially make mistakes, by
+ * picking CPUs that are not idle or even offline, the logic has been designed
+ * to handle these mistakes in favor of a more efficient response and a reduced
+ * scheduling overhead.
+ */
+static s32 pick_idle_cpu(struct task_struct *p, s32 prev_cpu, u64 enq_flags)
+{
+	const struct cpumask *online_cpumask, *idle_smtmask, *idle_cpumask;
+	struct bpf_cpumask *l2_domain, *l3_domain;
+	struct bpf_cpumask *l2_mask, *l3_mask;
+	struct task_ctx *tctx;
+	struct cpu_ctx *cctx;
+	s32 cpu;
+
+	/*
+	 * If the task isn't allowed to use its previously used CPU it means
+	 * that it's rapidly changing affinity. In this case it's pointless to
+	 * find an optimal idle CPU, just return and let the task being
+	 * dispatched to a global DSQ.
+	 */
+	if (!bpf_cpumask_test_cpu(prev_cpu, p->cpus_ptr))
+		return -ENOENT;
+
+	/*
+	 * For tasks that can run only on a single CPU, simply return the only
+	 * allowed CPU.
+	 */
+	if (p->nr_cpus_allowed == 1)
+		return prev_cpu;
+
+	tctx = try_lookup_task_ctx(p);
+	if (!tctx)
+		return -ENOENT;
+
+	cctx = try_lookup_cpu_ctx(prev_cpu);
+	if (!cctx)
+		return -ENOENT;
+
+	/*
+	 * Acquire the CPU masks to determine the online and idle CPUs in the
+	 * system.
+	 */
+	online_cpumask = scx_bpf_get_online_cpumask();
+	idle_smtmask = scx_bpf_get_idle_smtmask();
+	idle_cpumask = scx_bpf_get_idle_cpumask();
+
+	/*
+	 * Scheduling domains of the previously used CPU.
+	 */
+	l2_domain = cctx->l2_cpumask;
+	if (!l2_domain)
+		l2_domain = (struct bpf_cpumask *)p->cpus_ptr;
+
+	l3_domain = cctx->l3_cpumask;
+	if (!l3_domain)
+		l3_domain = (struct bpf_cpumask *)p->cpus_ptr;
+
+	/*
+	 * Task's scheduling domains.
+	 */
+	l2_mask = tctx->l2_cpumask;
+	if (!l2_mask) {
+		scx_bpf_error("l2 cpumask not initialized");
+		cpu = -ENOENT;
+		goto out_put_cpumask;
+	}
+	l3_mask = tctx->l3_cpumask;
+	if (!l3_mask) {
+		scx_bpf_error("l3 cpumask not initialized");
+		cpu = -ENOENT;
+		goto out_put_cpumask;
+	}
+
+	/*
+	 * Determine the L2 cache domain as the intersection of the task's
+	 * primary cpumask and the L2 cache domain mask of the previously used
+	 * CPU (ignore if this cpumask completely overlaps with the task's
+	 * cpumask).
+	 */
+	bpf_cpumask_and(l2_mask, p->cpus_ptr, cast_mask(l2_domain));
+
+	/*
+	 * Determine the L3 cache domain as the intersection of the task's
+	 * primary cpumask and the L3 cache domain mask of the previously used
+	 * CPU (ignore if this cpumask completely overlaps with the task's
+	 * cpumask).
+	 */
+	bpf_cpumask_and(l3_mask, p->cpus_ptr, cast_mask(l3_domain));
+
+	if (enq_flags & SCX_ENQ_WAKEUP) {
+		struct task_struct *current = (void *)bpf_get_current_task_btf();
+		bool share_llc, has_idle;
+
+		/*
+		 * Determine waker CPU scheduling domain.
+		 */
+		cpu = bpf_get_smp_processor_id();
+
+		cctx = try_lookup_cpu_ctx(cpu);
+		if (!cctx) {
+			cpu = -ENOENT;
+			goto out_put_cpumask;
+		}
+
+		l3_domain = cctx->l3_cpumask;
+		if (!l3_domain) {
+			scx_bpf_error("CPU LLC cpumask not initialized");
+			cpu = -ENOENT;
+			goto out_put_cpumask;
+		}
+
+		/*
+		 * If both the waker and wakee share the same LLC keep using
+		 * the same CPU if possible.
+		 */
+		share_llc = bpf_cpumask_test_cpu(prev_cpu, cast_mask(l3_domain));
+		if (share_llc && scx_bpf_test_and_clear_cpu_idle(prev_cpu)) {
+			cpu = prev_cpu;
+			goto out_put_cpumask;
+		}
+
+		/*
+		 * If the waker's domain is not saturated attempt to migrate
+		 * the wakee on the same CPU as the waker.
+		 */
+		has_idle = bpf_cpumask_intersects(cast_mask(l3_domain), idle_cpumask);
+		if (has_idle &&
+		    bpf_cpumask_test_cpu(cpu, p->cpus_ptr) &&
+		    !(current->flags & PF_EXITING) &&
+		    scx_bpf_dsq_nr_queued(cpu_to_dsq(cpu)) == 0)
+			goto out_put_cpumask;
+	}
+
+	/*
+	 * Find the best idle CPU, prioritizing full idle cores in SMT systems.
+	 */
+	if (smt_enabled) {
+		/*
+		 * If the task can still run on the previously used CPU and
+		 * it's a full-idle core, keep using it.
+		 */
+		if (bpf_cpumask_test_cpu(prev_cpu, p->cpus_ptr) &&
+		    bpf_cpumask_test_cpu(prev_cpu, idle_smtmask) &&
+		    scx_bpf_test_and_clear_cpu_idle(prev_cpu)) {
+			cpu = prev_cpu;
+			goto out_put_cpumask;
+		}
+
+		/*
+		 * Search for any full-idle CPU in the task domain that shares
+		 * the same L2 cache.
+		 */
+		cpu = bpf_cpumask_any_and_distribute(cast_mask(l2_mask), idle_smtmask);
+		if (bpf_cpumask_test_cpu(cpu, online_cpumask) &&
+		    scx_bpf_test_and_clear_cpu_idle(cpu))
+			goto out_put_cpumask;
+
+		/*
+		 * Search for any full-idle CPU in the task domain that shares
+		 * the same L3 cache.
+		 */
+		cpu = bpf_cpumask_any_and_distribute(cast_mask(l3_mask), idle_smtmask);
+		if (bpf_cpumask_test_cpu(cpu, online_cpumask) &&
+		    scx_bpf_test_and_clear_cpu_idle(cpu))
+			goto out_put_cpumask;
+
+		/*
+		 * Otherwise, search for another usable full-idle core.
+		 */
+		cpu = bpf_cpumask_any_and_distribute(p->cpus_ptr, idle_smtmask);
+		if (bpf_cpumask_test_cpu(cpu, online_cpumask) &&
+		    scx_bpf_test_and_clear_cpu_idle(cpu))
+			goto out_put_cpumask;
+	}
+
+	/*
+	 * If a full-idle core can't be found (or if this is not an SMT system)
+	 * try to re-use the same CPU, even if it's not in a full-idle core.
+	 */
+	if (bpf_cpumask_test_cpu(prev_cpu, p->cpus_ptr) &&
+	    scx_bpf_test_and_clear_cpu_idle(prev_cpu)) {
+		cpu = prev_cpu;
+		goto out_put_cpumask;
+	}
+
+	/*
+	 * Search for any idle CPU in the primary domain that shares the same
+	 * L2 cache.
+	 */
+	cpu = bpf_cpumask_any_and_distribute(cast_mask(l2_mask), idle_cpumask);
+	if (bpf_cpumask_test_cpu(cpu, online_cpumask) &&
+	    scx_bpf_test_and_clear_cpu_idle(cpu))
+		goto out_put_cpumask;
+
+	/*
+	 * Search for any idle CPU in the primary domain that shares the same
+	 * L3 cache.
+	 */
+	cpu = bpf_cpumask_any_and_distribute(cast_mask(l3_mask), idle_cpumask);
+	if (bpf_cpumask_test_cpu(cpu, online_cpumask) &&
+	    scx_bpf_test_and_clear_cpu_idle(cpu))
+		goto out_put_cpumask;
+
+	/*
+	 * If all the previous attempts have failed, try to use any idle CPU in
+	 * the system.
+	 */
+	cpu = bpf_cpumask_any_and_distribute(p->cpus_ptr, idle_cpumask);
+	if (bpf_cpumask_test_cpu(cpu, online_cpumask) &&
+	    scx_bpf_test_and_clear_cpu_idle(cpu))
+		goto out_put_cpumask;
+
+	/*
+	 * If all the previous attempts have failed, dispatch the task to the
+	 * first CPU that will become available.
+	 */
+	cpu = -ENOENT;
+
+out_put_cpumask:
+	scx_bpf_put_cpumask(idle_cpumask);
+	scx_bpf_put_cpumask(idle_smtmask);
+	scx_bpf_put_cpumask(online_cpumask);
+
+	return cpu;
+}
+
+/*
+ * Dispatch a task to a target per-CPU DSQ, waking up the corresponding CPU, if
+ * needed.
+ */
+static void dispatch_task(const struct dispatched_task_ctx *task)
+{
+	struct task_struct *p;
+	struct task_ctx *tctx;
+	u64 dsq_id, curr_cpumask_cnt;
+	s32 cpu;
+
+	/* Ignore entry if the task doesn't exist anymore */
+	p = bpf_task_from_pid(task->pid);
+	if (!p)
+		return;
+
+	/*
+	 * Update task's time slice in its context.
+	 */
+	tctx = try_lookup_task_ctx(p);
+	if (!tctx) {
+		/*
+		 * Bounce to the shared DSQ if we can't find a valid task
+		 * context.
+		 */
+		scx_bpf_dispatch_vtime(p, SHARED_DSQ,
+				       SCX_SLICE_DFL, task->vtime, task->flags);
+		__sync_fetch_and_add(&nr_bounce_dispatches, 1);
+		goto out_kick_idle_cpu;
+	}
+	tctx->slice_ns = task->slice_ns;
+
+	/*
+	 * Dispatch task to the target DSQ.
+	 */
+	if (task->cpu & RL_CPU_ANY) {
+		scx_bpf_dispatch_vtime(p, SHARED_DSQ,
+				       SCX_SLICE_DFL, task->vtime, task->flags);
+		goto out_kick_idle_cpu;
+	}
+
+	/*
+	 * Force tasks that are currently executing sched_setaffinity() to be
+	 * dispatched on the shared DSQ, otherwise we may introduce stalls in
+	 * the per-CPU DSQ.
+	 */
+	if (in_setaffinity(p->pid)) {
+		scx_bpf_dispatch_vtime(p, SHARED_DSQ,
+				       SCX_SLICE_DFL, task->vtime, task->flags);
+		__sync_fetch_and_add(&nr_bounce_dispatches, 1);
+		goto out_kick_idle_cpu;
+	}
+
+	/*
+	 * Dispatch a task to a specific per-CPU DSQ if the target CPU can be
+	 * used (according to the cpumask), otherwise redirect the task to the
+	 * shared DSQ.
+	 *
+	 * This can happen if the user-space scheduler dispatches the task to
+	 * an invalid CPU. In this case the redirection to the shared DSQ
+	 * allows to prevent potential stalls in the scheduler.
+	 *
+	 * If the cpumask is not valid anymore (determined by the cpumask_cnt
+	 * generation counter) we can simply cancel the dispatch event, since
+	 * the task will be re-enqueued by the core sched-ext code, potentially
+	 * selecting a different cpu and a different cpumask.
+	 */
+	dsq_id = cpu_to_dsq(task->cpu);
+
+	/* Check if the CPU is valid, according to the cpumask */
+	if (!bpf_cpumask_test_cpu(task->cpu, p->cpus_ptr)) {
+		scx_bpf_dispatch_vtime(p, SHARED_DSQ,
+				       SCX_SLICE_DFL, task->vtime, task->flags);
+		__sync_fetch_and_add(&nr_bounce_dispatches, 1);
+		goto out_kick_idle_cpu;
+	}
+
+	/* Read current cpumask generation counter */
+	curr_cpumask_cnt = tctx->cpumask_cnt;
+
+	/* Dispatch the task to the target per-CPU DSQ */
+	scx_bpf_dispatch_vtime(p, dsq_id,
+			       SCX_SLICE_DFL, task->vtime, task->flags);
+
+	/* If the cpumask is not valid anymore, ignore the dispatch event */
+	if (curr_cpumask_cnt != task->cpumask_cnt) {
+		scx_bpf_dispatch_cancel();
+		__sync_fetch_and_add(&nr_cancel_dispatches, 1);
+		goto out_release;
+	}
+
+	if (task->cpu != bpf_get_smp_processor_id())
+		scx_bpf_kick_cpu(task->cpu, SCX_KICK_IDLE);
+
+	goto out_release;
+
+out_kick_idle_cpu:
+	cpu = pick_idle_cpu(p, task->cpu, task->flags);
+	if (cpu >= 0)
+		scx_bpf_kick_cpu(cpu, 0);
+
+out_release:
+	bpf_task_release(p);
+}
+
+s32 BPF_STRUCT_OPS(rustland_select_cpu, struct task_struct *p, s32 prev_cpu,
+		   u64 wake_flags)
+{
+	/*
+	 * Completely delegate the CPU selection logic to the user-space
+	 * scheduler.
+	 */
+	return prev_cpu;
+}
+
+/*
+ * Select an idle CPU for a specific task from the user-space scheduler.
+ */
+SEC("syscall")
+int rs_select_cpu(struct task_cpu_arg *input)
+{
+	struct task_struct *p;
+	int cpu;
+
+	p = bpf_task_from_pid(input->pid);
+	if (!p)
+		return -EINVAL;
+	bpf_rcu_read_lock();
+	cpu = pick_idle_cpu(p, input->cpu, input->flags);
+	bpf_rcu_read_unlock();
+
+	bpf_task_release(p);
+
+	return cpu;
+}
+
+/*
+ * Fill @task with all the information that need to be sent to the user-space
+ * scheduler.
+ */
+static void get_task_info(struct queued_task_ctx *task,
+			  const struct task_struct *p, u64 enq_flags)
+{
+	struct task_ctx *tctx = try_lookup_task_ctx(p);
+
+	task->pid = p->pid;
+	task->sum_exec_runtime = p->se.sum_exec_runtime;
+	task->flags = enq_flags;
+	task->weight = p->scx.weight;
+	task->cpu = scx_bpf_task_cpu(p);
+	task->cpumask_cnt = tctx ? tctx->cpumask_cnt : 0;
+}
+
+/*
+ * User-space scheduler is congested: log that and increment congested counter.
+ */
+static void sched_congested(struct task_struct *p)
+{
+	dbg_msg("congested: pid=%d (%s)", p->pid, p->comm);
+	__sync_fetch_and_add(&nr_sched_congested, 1);
+}
+
+/*
+ * Task @p becomes ready to run. We can dispatch the task directly here if the
+ * user-space scheduler is not required, or enqueue it to be processed by the
+ * scheduler.
+ */
+void BPF_STRUCT_OPS(rustland_enqueue, struct task_struct *p, u64 enq_flags)
+{
+	struct queued_task_ctx *task;
+
+	/*
+	 * Scheduler is dispatched directly in .dispatch() when needed, so
+	 * we can skip it here.
+	 */
+	if (is_usersched_task(p))
+		return;
+
+	/*
+	 * Always dispatch all kthreads directly on their target CPU.
+	 *
+	 * This allows to prevent critical kernel threads from being stuck in
+	 * user-space causing system hangs.
+	 */
+	if (is_kthread(p)) {
+		s32 cpu = scx_bpf_task_cpu(p);
+
+		scx_bpf_dispatch_vtime(p, cpu_to_dsq(cpu),
+				       SCX_SLICE_DFL, 0, enq_flags);
+		__sync_fetch_and_add(&nr_kernel_dispatches, 1);
+		return;
+	}
+
+	/*
+	 * Add tasks to the @queued list, they will be processed by the
+	 * user-space scheduler.
+	 *
+	 * If @queued list is full (user-space scheduler is congested) tasks
+	 * will be dispatched directly from the kernel (using the first CPU
+	 * available in this case).
+	 */
+	task = bpf_ringbuf_reserve(&queued, sizeof(*task), 0);
+	if (!task) {
+		sched_congested(p);
+		scx_bpf_dispatch_vtime(p, SHARED_DSQ,
+				       SCX_SLICE_DFL, 0, enq_flags);
+		__sync_fetch_and_add(&nr_kernel_dispatches, 1);
+		return;
+	}
+	get_task_info(task, p, enq_flags);
+	dbg_msg("enqueue: pid=%d (%s)", p->pid, p->comm);
+	bpf_ringbuf_submit(task, 0);
+
+	__sync_fetch_and_add(&nr_queued, 1);
+}
+
+/*
+ * Dispatch the user-space scheduler.
+ */
+static bool dispatch_user_scheduler(void)
+{
+	struct task_struct *p;
+	s32 cpu;
+
+	if (!test_and_clear_usersched_needed())
+		return false;
+
+	p = bpf_task_from_pid(usersched_pid);
+	if (!p) {
+		scx_bpf_error("Failed to find usersched task %d", usersched_pid);
+		return false;
+	}
+	/*
+	 * Use the highest vtime possible to give the scheduler itself the
+	 * lowest priority possible.
+	 */
+	scx_bpf_dispatch_vtime(p, SHARED_DSQ, SCX_SLICE_DFL, -1ULL, 0);
+
+	cpu = pick_idle_cpu(p, scx_bpf_task_cpu(p), 0);
+	if (cpu >= 0)
+		scx_bpf_kick_cpu(cpu, 0);
+	bpf_task_release(p);
+
+	return true;
+}
+
+/*
+ * Handle a task dispatched from user-space, performing the actual low-level
+ * BPF dispatch.
+ */
+static long handle_dispatched_task(struct bpf_dynptr *dynptr, void *context)
+{
+	const struct dispatched_task_ctx *task;
+
+	task = bpf_dynptr_data(dynptr, 0, sizeof(*task));
+	if (!task)
+		return 0;
+
+	dispatch_task(task);
+	__sync_fetch_and_add(&nr_user_dispatches, 1);
+
+	return !!scx_bpf_dispatch_nr_slots();
+}
+
+/*
+ * Consume tasks dispatched to CPUs that have gone offline.
+ *
+ * These tasks will be consumed on other active CPUs to prevent indefinite
+ * stalling.
+ *
+ * Return true if one task is consumed, false otherwise.
+ */
+static bool consume_offline_cpus(s32 cpu)
+{
+	u64 nr_cpu_ids = scx_bpf_nr_cpu_ids();
+	struct bpf_cpumask *offline;
+	bool ret = false;
+
+	if (!test_and_clear_offline_needed())
+		return false;
+
+	offline = offline_cpumask;
+	if (!offline)
+		return false;
+
+	/*
+	 * Cycle through all the CPUs and evenly consume tasks from the DSQs of
+	 * those that are offline.
+	 */
+	bpf_repeat(nr_cpu_ids - 1) {
+		cpu = (cpu + 1) % nr_cpu_ids;
+
+		if (!bpf_cpumask_test_cpu(cpu, cast_mask(offline)))
+			continue;
+		/*
+		 * This CPU is offline, if a task has been dispatched there
+		 * consume it immediately on the current CPU.
+		 */
+		if (scx_bpf_consume(cpu_to_dsq(cpu))) {
+			set_offline_needed();
+			ret = true;
+			break;
+		}
+	}
+
+	return ret;
+}
+
+/*
+ * Dispatch tasks that are ready to run.
+ *
+ * This function is called when a CPU's local DSQ is empty and ready to accept
+ * new dispatched tasks.
+ *
+ * We may dispatch tasks also on other CPUs from here, if the scheduler decided
+ * so (usually if other CPUs are idle we may want to send more tasks to their
+ * local DSQ to optimize the scheduling pipeline).
+ */
+void BPF_STRUCT_OPS(rustland_dispatch, s32 cpu, struct task_struct *prev)
+{
+	/*
+	 * Consume all tasks from the @dispatched list and immediately dispatch
+	 * them on the target CPU decided by the user-space scheduler.
+	 */
+	bpf_user_ringbuf_drain(&dispatched, handle_dispatched_task, NULL, 0);
+
+	/*
+	 * Try to steal a task dispatched to CPUs that may have gone offline
+	 * (this allows to prevent indefinite task stalls).
+	 */
+	if (consume_offline_cpus(cpu))
+		return;
+
+	/*
+	 * Consume a task from the per-CPU DSQ.
+	 */
+	if (scx_bpf_consume(cpu_to_dsq(cpu)))
+		return;
+
+	/*
+	 * Consume a task from the shared DSQ.
+	 */
+	if (scx_bpf_consume(SHARED_DSQ))
+		return;
+
+	/*
+	 * Check if the user-space scheduler needs to run.
+	 */
+	dispatch_user_scheduler();
+}
+
+/*
+ * Task @p starts on its selected CPU (update CPU ownership map).
+ */
+void BPF_STRUCT_OPS(rustland_running, struct task_struct *p)
+{
+	s32 cpu = scx_bpf_task_cpu(p);
+
+	dbg_msg("start: pid=%d (%s) cpu=%ld", p->pid, p->comm, cpu);
+
+	/*
+	 * Ensure time slice never exceeds slice_ns when a task is started on a
+	 * CPU.
+	 */
+	p->scx.slice = task_slice(p);
+
+	/*
+	 * Mark the CPU as busy by setting the pid as owner (ignoring the
+	 * user-space scheduler).
+	 */
+	if (!is_usersched_task(p))
+		__sync_fetch_and_add(&nr_running, 1);
+}
+
+/*
+ * Task @p stops running on its associated CPU (update CPU ownership map).
+ */
+void BPF_STRUCT_OPS(rustland_stopping, struct task_struct *p, bool runnable)
+{
+	s32 cpu = scx_bpf_task_cpu(p);
+
+	dbg_msg("stop: pid=%d (%s) cpu=%ld", p->pid, p->comm, cpu);
+	/*
+	 * Mark the CPU as idle by setting the owner to 0.
+	 */
+	if (!is_usersched_task(p)) {
+		__sync_fetch_and_sub(&nr_running, 1);
+		/*
+		 * Kick the user-space scheduler immediately when a task
+		 * releases a CPU and speculate on the fact that most of the
+		 * time there is another task ready to run.
+		 */
+		set_usersched_needed();
+	}
+}
+
+/*
+ * A CPU is about to change its idle state.
+ */
+void BPF_STRUCT_OPS(rustland_update_idle, s32 cpu, bool idle)
+{
+	/*
+	 * Don't do anything if we exit from and idle state, a CPU owner will
+	 * be assigned in .running().
+	 */
+	if (!idle)
+		return;
+	/*
+	 * A CPU is now available, notify the user-space scheduler that tasks
+	 * can be dispatched.
+	 */
+	if (usersched_has_pending_tasks()) {
+		set_usersched_needed();
+		/*
+		 * Wake up the idle CPU and trigger a resched, so that it can
+		 * immediately accept dispatched tasks.
+		 */
+		scx_bpf_kick_cpu(cpu, 0);
+		return;
+	}
+
+	/*
+	 * Kick the CPU if there are still tasks dispatched to the
+	 * corresponding per-CPU DSQ.
+	 */
+	if (scx_bpf_dsq_nr_queued(cpu_to_dsq(cpu)) > 0)
+		scx_bpf_kick_cpu(cpu, 0);
+}
+
+/*
+ * Task @p changes cpumask: update its local cpumask generation counter.
+ */
+void BPF_STRUCT_OPS(rustland_set_cpumask, struct task_struct *p,
+		    const struct cpumask *cpumask)
+{
+	struct task_ctx *tctx;
+
+	tctx = try_lookup_task_ctx(p);
+	if (!tctx)
+		return;
+	tctx->cpumask_cnt++;
+}
+
+/*
+ * A CPU is taken away from the scheduler, preempting the current task by
+ * another one running in a higher priority sched_class.
+ */
+void BPF_STRUCT_OPS(rustland_cpu_release, s32 cpu,
+				struct scx_cpu_release_args *args)
+{
+	struct task_struct *p = args->task;
+	/*
+	 * If the interrupted task is the user-space scheduler make sure to
+	 * re-schedule it immediately.
+	 */
+	dbg_msg("cpu preemption: pid=%d (%s)", p->pid, p->comm);
+	if (is_usersched_task(p))
+		set_usersched_needed();
+}
+
+void BPF_STRUCT_OPS(rustland_cpu_online, s32 cpu)
+{
+	/* Set the CPU state to online */
+	set_cpu_state(offline_cpumask, cpu, false);
+
+	__sync_fetch_and_add(&nr_online_cpus, 1);
+	__sync_fetch_and_add(&cpu_hotplug_cnt, 1);
+}
+
+void BPF_STRUCT_OPS(rustland_cpu_offline, s32 cpu)
+{
+	/* Set the CPU state to offline */
+	set_cpu_state(offline_cpumask, cpu, true);
+
+	__sync_fetch_and_sub(&nr_online_cpus, 1);
+	__sync_fetch_and_add(&cpu_hotplug_cnt, 1);
+
+	set_offline_needed();
+}
+
+/*
+ * A new task @p is being created.
+ *
+ * Allocate and initialize all the internal structures for the task (this
+ * function is allowed to block, so it can be used to preallocate memory).
+ */
+s32 BPF_STRUCT_OPS(rustland_init_task, struct task_struct *p,
+		   struct scx_init_task_args *args)
+{
+	struct task_ctx *tctx;
+	struct bpf_cpumask *cpumask;
+
+	tctx = bpf_task_storage_get(&task_ctx_stor, p, 0,
+				    BPF_LOCAL_STORAGE_GET_F_CREATE);
+	if (!tctx)
+		return -ENOMEM;
+	tctx->slice_ns = SCX_SLICE_DFL;
+
+	/*
+	 * Create task's L2 cache cpumask.
+	 */
+	cpumask = bpf_cpumask_create();
+	if (!cpumask)
+		return -ENOMEM;
+	cpumask = bpf_kptr_xchg(&tctx->l2_cpumask, cpumask);
+	if (cpumask)
+		bpf_cpumask_release(cpumask);
+
+	/*
+	 * Create task's L3 cache cpumask.
+	 */
+	cpumask = bpf_cpumask_create();
+	if (!cpumask)
+		return -ENOMEM;
+	cpumask = bpf_kptr_xchg(&tctx->l3_cpumask, cpumask);
+	if (cpumask)
+		bpf_cpumask_release(cpumask);
+
+	return 0;
+}
+
+/*
+ * Heartbeat scheduler timer callback.
+ *
+ * If the system is completely idle the sched-ext watchdog may incorrectly
+ * detect that as a stall and automatically disable the scheduler. So, use this
+ * timer to periodically wake-up the scheduler and avoid long inactivity.
+ *
+ * This can also help to prevent real "stalling" conditions in the scheduler.
+ */
+static int usersched_timer_fn(void *map, int *key, struct bpf_timer *timer)
+{
+	int err = 0;
+
+	/* Kick the scheduler */
+	set_usersched_needed();
+
+	/* Re-arm the timer */
+	err = bpf_timer_start(timer, USERSCHED_TIMER_NS, 0);
+	if (err)
+		scx_bpf_error("Failed to arm stats timer");
+
+	return 0;
+}
+
+/*
+ * Initialize the heartbeat scheduler timer.
+ */
+static int usersched_timer_init(void)
+{
+	struct bpf_timer *timer;
+	u32 key = 0;
+	int err;
+
+	timer = bpf_map_lookup_elem(&usersched_timer, &key);
+	if (!timer) {
+		scx_bpf_error("Failed to lookup scheduler timer");
+		return -ESRCH;
+	}
+	bpf_timer_init(timer, &usersched_timer, CLOCK_BOOTTIME);
+	bpf_timer_set_callback(timer, usersched_timer_fn);
+	err = bpf_timer_start(timer, USERSCHED_TIMER_NS, 0);
+	if (err)
+		scx_bpf_error("Failed to arm scheduler timer");
+
+	return err;
+}
+
+/*
+ * Evaluate the amount of online CPUs.
+ */
+s32 get_nr_online_cpus(void)
+{
+	const struct cpumask *online_cpumask;
+	u64 nr_cpu_ids = scx_bpf_nr_cpu_ids();
+	int i, cpus = 0;
+
+	online_cpumask = scx_bpf_get_online_cpumask();
+
+	bpf_for(i, 0, nr_cpu_ids) {
+		if (!bpf_cpumask_test_cpu(i, online_cpumask))
+			continue;
+		cpus++;
+	}
+
+	scx_bpf_put_cpumask(online_cpumask);
+
+	return cpus;
+}
+
+/*
+ * Create a DSQ for each CPU available in the system and a global shared DSQ.
+ *
+ * All the tasks processed by the user-space scheduler can be dispatched either
+ * to a specific CPU/DSQ or to the first CPU available (SHARED_DSQ).
+ *
+ * Custom DSQs are then consumed from the .dispatch() callback, that will
+ * transfer all the enqueued tasks to the consuming CPU's local DSQ.
+ */
+static int dsq_init(void)
+{
+	u64 nr_cpu_ids = scx_bpf_nr_cpu_ids();
+	int err;
+	s32 cpu;
+
+	/* Initialize amount of online CPUs */
+	nr_online_cpus = get_nr_online_cpus();
+
+	/* Create per-CPU DSQs */
+	bpf_for(cpu, 0, nr_cpu_ids) {
+		err = scx_bpf_create_dsq(cpu_to_dsq(cpu), -1);
+		if (err) {
+			scx_bpf_error("failed to create pcpu DSQ %d: %d",
+				      cpu, err);
+			return err;
+		}
+	}
+
+	/* Create the global shared DSQ */
+	err = scx_bpf_create_dsq(SHARED_DSQ, -1);
+	if (err) {
+		scx_bpf_error("failed to create shared DSQ: %d", err);
+		return err;
+	}
+
+	return 0;
+}
+
+static int init_cpumask(struct bpf_cpumask **cpumask)
+{
+	struct bpf_cpumask *mask;
+	int err = 0;
+
+	/*
+	 * Do nothing if the mask is already initialized.
+	 */
+	mask = *cpumask;
+	if (mask)
+		return 0;
+	/*
+	 * Create the CPU mask.
+	 */
+	err = calloc_cpumask(cpumask);
+	if (!err)
+		mask = *cpumask;
+	if (!mask)
+		err = -ENOMEM;
+
+	return err;
+}
+
+SEC("syscall")
+int enable_sibling_cpu(struct domain_arg *input)
+{
+	struct cpu_ctx *cctx;
+	struct bpf_cpumask *mask, **pmask;
+	int err = 0;
+
+	cctx = try_lookup_cpu_ctx(input->cpu_id);
+	if (!cctx)
+		return -ENOENT;
+
+	/* Make sure the target CPU mask is initialized */
+	switch (input->lvl_id) {
+	case 2:
+		pmask = &cctx->l2_cpumask;
+		break;
+	case 3:
+		pmask = &cctx->l3_cpumask;
+		break;
+	default:
+		return -EINVAL;
+	}
+	err = init_cpumask(pmask);
+	if (err)
+		return err;
+
+	bpf_rcu_read_lock();
+	mask = *pmask;
+	if (mask)
+		bpf_cpumask_set_cpu(input->sibling_cpu_id, mask);
+	bpf_rcu_read_unlock();
+
+	return err;
+}
+
+/*
+ * Initialize the scheduling class.
+ */
+s32 BPF_STRUCT_OPS_SLEEPABLE(rustland_init)
+{
+	struct bpf_cpumask *mask;
+	int err;
+
+	/* Compile-time checks */
+	BUILD_BUG_ON((MAX_CPUS % 2));
+
+	/* Initialize the offline CPU mask */
+	err = calloc_cpumask(&offline_cpumask);
+	mask = offline_cpumask;
+	if (!mask)
+		err = -ENOMEM;
+	if (err)
+		return err;
+
+	/* Initialize rustland core */
+	err = dsq_init();
+	if (err)
+		return err;
+	err = usersched_timer_init();
+	if (err)
+		return err;
+
+	return 0;
+}
+
+/*
+ * Unregister the scheduling class.
+ */
+void BPF_STRUCT_OPS(rustland_exit, struct scx_exit_info *ei)
+{
+	UEI_RECORD(uei, ei);
+}
+
+/*
+ * Scheduling class declaration.
+ */
+SCX_OPS_DEFINE(rustland,
+	       .select_cpu		= (void *)rustland_select_cpu,
+	       .enqueue			= (void *)rustland_enqueue,
+	       .dispatch		= (void *)rustland_dispatch,
+	       .running			= (void *)rustland_running,
+	       .stopping		= (void *)rustland_stopping,
+	       .update_idle		= (void *)rustland_update_idle,
+	       .set_cpumask		= (void *)rustland_set_cpumask,
+	       .cpu_release		= (void *)rustland_cpu_release,
+	       .cpu_online		= (void *)rustland_cpu_online,
+	       .cpu_offline		= (void *)rustland_cpu_offline,
+	       .init_task		= (void *)rustland_init_task,
+	       .init			= (void *)rustland_init,
+	       .exit			= (void *)rustland_exit,
+	       .flags			= SCX_OPS_ENQ_LAST | SCX_OPS_KEEP_BUILTIN_IDLE,
+	       .timeout_ms		= 5000,
+	       .dispatch_max_batch	= MAX_DISPATCH_SLOT,
+	       .name			= "rustland");
--- /dev/null
+++ scx-1.0.4.1/scheds/rust/scx_rustland/src/bpf.rs
@@ -0,0 +1,592 @@
+// Copyright (c) Andrea Righi <andrea.righi@linux.dev>
+
+// This software may be used and distributed according to the terms of the
+// GNU General Public License version 2.
+
+use std::mem::MaybeUninit;
+
+use crate::bpf_intf;
+use crate::bpf_intf::*;
+use crate::bpf_skel::*;
+
+use std::ffi::c_int;
+use std::ffi::c_ulong;
+use std::fs::File;
+use std::io::Read;
+
+use std::collections::HashMap;
+use std::sync::atomic::AtomicBool;
+use std::sync::atomic::Ordering;
+use std::sync::Arc;
+
+use anyhow::Context;
+use anyhow::Result;
+
+use plain::Plain;
+
+use libbpf_rs::skel::OpenSkel;
+use libbpf_rs::skel::Skel;
+use libbpf_rs::skel::SkelBuilder;
+use libbpf_rs::OpenObject;
+use libbpf_rs::ProgramInput;
+
+use libc::{pthread_self, pthread_setschedparam, sched_param};
+
+#[cfg(target_env = "musl")]
+use libc::timespec;
+
+use scx_utils::compat;
+use scx_utils::scx_ops_attach;
+use scx_utils::scx_ops_load;
+use scx_utils::scx_ops_open;
+use scx_utils::uei_exited;
+use scx_utils::uei_report;
+use scx_utils::Topology;
+use scx_utils::UserExitInfo;
+
+use scx_rustland_core::ALLOCATOR;
+
+// Defined in UAPI
+const SCHED_EXT: i32 = 7;
+
+// Allow to dispatch the task on any CPU.
+//
+// The task will be dispatched to the global shared DSQ and it will run on the first CPU available.
+#[allow(dead_code)]
+pub const RL_CPU_ANY: i32 = bpf_intf::RL_CPU_ANY as i32;
+
+/// High-level Rust abstraction to interact with a generic sched-ext BPF component.
+///
+/// Overview
+/// ========
+///
+/// The main BPF interface is provided by the BpfScheduler() struct. When this object is
+/// initialized it will take care of registering and initializing the BPF component.
+///
+/// The scheduler then can use BpfScheduler() instance to receive tasks (in the form of QueuedTask
+/// objects) and dispatch tasks (in the form of DispatchedTask objects), using respectively the
+/// methods dequeue_task() and dispatch_task().
+///
+/// BPF counters and statistics can be accessed using the methods nr_*_mut(), in particular
+/// nr_queued_mut() and nr_scheduled_mut() can be updated to notify the BPF component if the
+/// user-space scheduler has some pending work to do or not.
+///
+/// Finally the methods exited() and shutdown_and_report() can be used respectively to test
+/// whether the BPF component exited, and to shutdown and report the exit message.
+/// whether the BPF component exited, and to shutdown and report exit message.
+
+// Task queued for scheduling from the BPF component (see bpf_intf::queued_task_ctx).
+#[derive(Debug, PartialEq, Eq, PartialOrd, Clone)]
+pub struct QueuedTask {
+    pub pid: i32,              // pid that uniquely identifies a task
+    pub cpu: i32,              // CPU where the task is running
+    pub flags: u64,            // task enqueue flags
+    pub sum_exec_runtime: u64, // Total cpu time
+    pub weight: u64,           // Task static priority
+    cpumask_cnt: u64,          // cpumask generation counter (private)
+}
+
+// Task queued for dispatching to the BPF component (see bpf_intf::dispatched_task_ctx).
+#[derive(Debug, PartialEq, Eq, PartialOrd, Clone)]
+pub struct DispatchedTask {
+    pub pid: i32,      // pid that uniquely identifies a task
+    pub cpu: i32,      // target CPU selected by the scheduler
+    pub flags: u64,    // special dispatch flags
+    pub slice_ns: u64, // time slice assigned to the task (0 = default)
+    pub vtime: u64,    // task deadline / vruntime
+    cpumask_cnt: u64,  // cpumask generation counter (private)
+}
+
+impl DispatchedTask {
+    // Create a DispatchedTask from a QueuedTask.
+    //
+    // A dispatched task should be always originated from a QueuedTask (there is no reason to
+    // dispatch a task if it wasn't queued to the scheduler earlier).
+    pub fn new(task: &QueuedTask) -> Self {
+        DispatchedTask {
+            pid: task.pid,
+            cpu: task.cpu,
+            flags: task.flags,
+            cpumask_cnt: task.cpumask_cnt,
+            slice_ns: 0, // use default time slice
+            vtime: 0,
+        }
+    }
+}
+
+// Helpers used to submit tasks to the BPF user ring buffer.
+unsafe impl Plain for bpf_intf::dispatched_task_ctx {}
+
+impl AsMut<bpf_intf::dispatched_task_ctx> for bpf_intf::dispatched_task_ctx {
+    fn as_mut(&mut self) -> &mut bpf_intf::dispatched_task_ctx {
+        self
+    }
+}
+
+// Message received from the dispatcher (see bpf_intf::queued_task_ctx for details).
+//
+// NOTE: eventually libbpf-rs will provide a better abstraction for this.
+struct EnqueuedMessage {
+    inner: bpf_intf::queued_task_ctx,
+}
+
+impl EnqueuedMessage {
+    fn from_bytes(bytes: &[u8]) -> Self {
+        let queued_task_struct = unsafe { *(bytes.as_ptr() as *const bpf_intf::queued_task_ctx) };
+        EnqueuedMessage {
+            inner: queued_task_struct,
+        }
+    }
+
+    fn to_queued_task(&self) -> QueuedTask {
+        QueuedTask {
+            pid: self.inner.pid,
+            cpu: self.inner.cpu,
+            flags: self.inner.flags,
+            sum_exec_runtime: self.inner.sum_exec_runtime,
+            weight: self.inner.weight,
+            cpumask_cnt: self.inner.cpumask_cnt,
+        }
+    }
+}
+
+pub struct BpfScheduler<'cb> {
+    pub skel: BpfSkel<'cb>,                // Low-level BPF connector
+    shutdown: Arc<AtomicBool>,             // Determine scheduler shutdown
+    queued: libbpf_rs::RingBuffer<'cb>,    // Ring buffer of queued tasks
+    dispatched: libbpf_rs::UserRingBuffer, // User Ring buffer of dispatched tasks
+    cpu_hotplug_cnt: u64,                  // CPU hotplug generation counter
+    struct_ops: Option<libbpf_rs::Link>,   // Low-level BPF methods
+}
+
+// Buffer to store a task read from the ring buffer.
+//
+// NOTE: make the buffer aligned to 64-bits to prevent misaligned dereferences when accessing the
+// buffer using a pointer.
+const BUFSIZE: usize = std::mem::size_of::<QueuedTask>();
+
+#[repr(align(8))]
+struct AlignedBuffer([u8; BUFSIZE]);
+
+static mut BUF: AlignedBuffer = AlignedBuffer([0; BUFSIZE]);
+
+// Special negative error code for libbpf to stop after consuming just one item from a BPF
+// ring buffer.
+const LIBBPF_STOP: i32 = -255;
+
+fn is_smt_active() -> std::io::Result<bool> {
+    let mut file = File::open("/sys/devices/system/cpu/smt/active")?;
+    let mut contents = String::new();
+    file.read_to_string(&mut contents)?;
+
+    let smt_active: i32 = contents.trim().parse().unwrap_or(0);
+
+    Ok(smt_active == 1)
+}
+
+impl<'cb> BpfScheduler<'cb> {
+    pub fn init(
+        open_object: &'cb mut MaybeUninit<OpenObject>,
+        exit_dump_len: u32,
+        partial: bool,
+        debug: bool,
+    ) -> Result<Self> {
+        let shutdown = Arc::new(AtomicBool::new(false));
+        let shutdown_clone = shutdown.clone();
+        ctrlc::set_handler(move || {
+            shutdown_clone.store(true, Ordering::Relaxed);
+        })
+        .context("Error setting Ctrl-C handler")?;
+
+        // Open the BPF prog first for verification.
+        let mut skel_builder = BpfSkelBuilder::default();
+        skel_builder.obj_builder.debug(debug);
+        let mut skel = scx_ops_open!(skel_builder, open_object, rustland)?;
+
+        // Lock all the memory to prevent page faults that could trigger potential deadlocks during
+        // scheduling.
+        ALLOCATOR.lock_memory();
+
+        // Copy one item from the ring buffer.
+        //
+        // # Safety
+        //
+        // Each invocation of the callback will trigger the copy of exactly one QueuedTask item to
+        // BUF. The caller must be synchronize to ensure that multiple invocations of the callback
+        // are not happening at the same time, but this is implicitly guaranteed by the fact that
+        // the caller is a single-thread process (for now).
+        //
+        // Use of a `str` whose contents are not valid UTF-8 is undefined behavior.
+        fn callback(data: &[u8]) -> i32 {
+            unsafe {
+                // SAFETY: copying from the BPF ring buffer to BUF is safe, since the size of BUF
+                // is exactly the size of QueuedTask and the callback operates in chunks of
+                // QueuedTask items. It also copies exactly one QueuedTask at a time, this is
+                // guaranteed by the error code returned by this callback (see below). From a
+                // thread-safety perspective this is also correct, assuming the caller is a
+                // single-thread process (as it is for now).
+                BUF.0.copy_from_slice(data);
+            }
+
+            // Return an unsupported error to stop early and consume only one item.
+            //
+            // NOTE: this is quite a hack. I wish libbpf would honor stopping after the first item
+            // is consumed, upon returning a non-zero positive value here, but it doesn't seem to
+            // be the case:
+            //
+            // https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/tree/tools/lib/bpf/ringbuf.c?h=v6.8-rc5#n260
+            //
+            // Maybe we should fix this to stop processing items from the ring buffer also when a
+            // value > 0 is returned.
+            //
+            LIBBPF_STOP
+        }
+
+        // Check host topology to determine if we need to enable SMT capabilities.
+        skel.maps.rodata_data.smt_enabled = is_smt_active()?;
+
+        // Set scheduler options (defined in the BPF part).
+        if partial {
+            skel.struct_ops.rustland_mut().flags |= *compat::SCX_OPS_SWITCH_PARTIAL;
+        }
+        skel.struct_ops.rustland_mut().exit_dump_len = exit_dump_len;
+
+        skel.maps.bss_data.usersched_pid = std::process::id();
+        skel.maps.rodata_data.debug = debug;
+
+        // Attach BPF scheduler.
+        let mut skel = scx_ops_load!(skel, rustland, uei)?;
+
+        // Initialize cache domains.
+        let topo = Topology::new().unwrap();
+        Self::init_l2_cache_domains(&mut skel, &topo)?;
+        Self::init_l3_cache_domains(&mut skel, &topo)?;
+
+        let struct_ops = Some(scx_ops_attach!(skel, rustland)?);
+
+        // Build the ring buffer of queued tasks.
+        let maps = &skel.maps;
+        let queued_ring_buffer = &maps.queued;
+        let mut rbb = libbpf_rs::RingBufferBuilder::new();
+        rbb.add(queued_ring_buffer, callback)
+            .expect("failed to add ringbuf callback");
+        let queued = rbb.build().expect("failed to build ringbuf");
+
+        // Build the user ring buffer of dispatched tasks.
+        let dispatched = libbpf_rs::UserRingBuffer::new(&maps.dispatched)
+            .expect("failed to create user ringbuf");
+
+        // Make sure to use the SCHED_EXT class at least for the scheduler itself.
+        match Self::use_sched_ext() {
+            0 => Ok(Self {
+                skel,
+                shutdown,
+                queued,
+                dispatched,
+                cpu_hotplug_cnt: 0,
+                struct_ops,
+            }),
+            err => Err(anyhow::Error::msg(format!(
+                "sched_setscheduler error: {}",
+                err
+            ))),
+        }
+    }
+
+    fn enable_sibling_cpu(
+        skel: &mut BpfSkel<'_>,
+        lvl: usize,
+        cpu: usize,
+        sibling_cpu: usize,
+    ) -> Result<(), u32> {
+        let prog = &mut skel.progs.enable_sibling_cpu;
+        let mut args = domain_arg {
+            lvl_id: lvl as c_int,
+            cpu_id: cpu as c_int,
+            sibling_cpu_id: sibling_cpu as c_int,
+        };
+        let input = ProgramInput {
+            context_in: Some(unsafe {
+                std::slice::from_raw_parts_mut(
+                    &mut args as *mut _ as *mut u8,
+                    std::mem::size_of_val(&args),
+                )
+            }),
+            ..Default::default()
+        };
+        let out = prog.test_run(input).unwrap();
+        if out.return_value != 0 {
+            return Err(out.return_value);
+        }
+
+        Ok(())
+    }
+
+    fn init_cache_domains(
+        skel: &mut BpfSkel<'_>,
+        topo: &Topology,
+        cache_lvl: usize,
+        enable_sibling_cpu_fn: &dyn Fn(&mut BpfSkel<'_>, usize, usize, usize) -> Result<(), u32>,
+    ) -> Result<(), std::io::Error> {
+        // Determine the list of CPU IDs associated to each cache node.
+        let mut cache_id_map: HashMap<usize, Vec<usize>> = HashMap::new();
+        for core in topo.cores().into_iter() {
+            for (cpu_id, cpu) in core.cpus() {
+                let cache_id = match cache_lvl {
+                    2 => cpu.l2_id(),
+                    3 => cpu.l3_id(),
+                    _ => panic!("invalid cache level {}", cache_lvl),
+                };
+                cache_id_map
+                    .entry(cache_id)
+                    .or_insert_with(Vec::new)
+                    .push(*cpu_id);
+            }
+        }
+
+        // Update the BPF cpumasks for the cache domains.
+        for (_cache_id, cpus) in cache_id_map {
+            for cpu in &cpus {
+                for sibling_cpu in &cpus {
+                    match enable_sibling_cpu_fn(skel, cache_lvl, *cpu, *sibling_cpu) {
+                        Ok(()) => {}
+                        Err(_) => {}
+                    }
+                }
+            }
+        }
+
+        Ok(())
+    }
+
+    fn init_l2_cache_domains(
+        skel: &mut BpfSkel<'_>,
+        topo: &Topology,
+    ) -> Result<(), std::io::Error> {
+        Self::init_cache_domains(skel, topo, 2, &|skel, lvl, cpu, sibling_cpu| {
+            Self::enable_sibling_cpu(skel, lvl, cpu, sibling_cpu)
+        })
+    }
+
+    fn init_l3_cache_domains(
+        skel: &mut BpfSkel<'_>,
+        topo: &Topology,
+    ) -> Result<(), std::io::Error> {
+        Self::init_cache_domains(skel, topo, 3, &|skel, lvl, cpu, sibling_cpu| {
+            Self::enable_sibling_cpu(skel, lvl, cpu, sibling_cpu)
+        })
+    }
+
+    fn refresh_cache_domains(&mut self) {
+        // Check if we need to refresh the CPU cache information.
+        if self.cpu_hotplug_cnt == self.skel.maps.bss_data.cpu_hotplug_cnt {
+            return;
+        }
+
+        // Re-initialize cache domains.
+        let topo = Topology::new().unwrap();
+        Self::init_l2_cache_domains(&mut self.skel, &topo).unwrap();
+        Self::init_l3_cache_domains(&mut self.skel, &topo).unwrap();
+
+        // Update CPU hotplug generation counter.
+        self.cpu_hotplug_cnt = self.skel.maps.bss_data.cpu_hotplug_cnt;
+    }
+
+    // Notify the BPF component that the user-space scheduler has completed its scheduling cycle,
+    // updating the amount tasks that are still peding.
+    //
+    // NOTE: do not set allow(dead_code) for this method, any scheduler must use this method at
+    // some point, otherwise the BPF component will keep waking-up the user-space scheduler in a
+    // busy loop, causing unnecessary high CPU consumption.
+    pub fn notify_complete(&mut self, nr_pending: u64) {
+        self.refresh_cache_domains();
+        self.skel.maps.bss_data.nr_scheduled = nr_pending;
+        std::thread::yield_now();
+    }
+
+    // Counter of the online CPUs.
+    #[allow(dead_code)]
+    pub fn nr_online_cpus_mut(&mut self) -> &mut u64 {
+        &mut self.skel.maps.bss_data.nr_online_cpus
+    }
+
+    // Counter of currently running tasks.
+    #[allow(dead_code)]
+    pub fn nr_running_mut(&mut self) -> &mut u64 {
+        &mut self.skel.maps.bss_data.nr_running
+    }
+
+    // Counter of queued tasks.
+    #[allow(dead_code)]
+    pub fn nr_queued_mut(&mut self) -> &mut u64 {
+        &mut self.skel.maps.bss_data.nr_queued
+    }
+
+    // Counter of scheduled tasks.
+    #[allow(dead_code)]
+    pub fn nr_scheduled_mut(&mut self) -> &mut u64 {
+        &mut self.skel.maps.bss_data.nr_scheduled
+    }
+
+    // Counter of user dispatch events.
+    #[allow(dead_code)]
+    pub fn nr_user_dispatches_mut(&mut self) -> &mut u64 {
+        &mut self.skel.maps.bss_data.nr_user_dispatches
+    }
+
+    // Counter of user kernel events.
+    #[allow(dead_code)]
+    pub fn nr_kernel_dispatches_mut(&mut self) -> &mut u64 {
+        &mut self.skel.maps.bss_data.nr_kernel_dispatches
+    }
+
+    // Counter of cancel dispatch events.
+    #[allow(dead_code)]
+    pub fn nr_cancel_dispatches_mut(&mut self) -> &mut u64 {
+        &mut self.skel.maps.bss_data.nr_cancel_dispatches
+    }
+
+    // Counter of dispatches bounced to the shared DSQ.
+    #[allow(dead_code)]
+    pub fn nr_bounce_dispatches_mut(&mut self) -> &mut u64 {
+        &mut self.skel.maps.bss_data.nr_bounce_dispatches
+    }
+
+    // Counter of failed dispatch events.
+    #[allow(dead_code)]
+    pub fn nr_failed_dispatches_mut(&mut self) -> &mut u64 {
+        &mut self.skel.maps.bss_data.nr_failed_dispatches
+    }
+
+    // Counter of scheduler congestion events.
+    #[allow(dead_code)]
+    pub fn nr_sched_congested_mut(&mut self) -> &mut u64 {
+        &mut self.skel.maps.bss_data.nr_sched_congested
+    }
+
+    // Set scheduling class for the scheduler itself to SCHED_EXT
+    fn use_sched_ext() -> i32 {
+        #[cfg(target_env = "gnu")]
+        let param: sched_param = sched_param { sched_priority: 0 };
+        #[cfg(target_env = "musl")]
+        let param: sched_param = sched_param {
+            sched_priority: 0,
+            sched_ss_low_priority: 0,
+            sched_ss_repl_period: timespec {
+                tv_sec: 0,
+                tv_nsec: 0,
+            },
+            sched_ss_init_budget: timespec {
+                tv_sec: 0,
+                tv_nsec: 0,
+            },
+            sched_ss_max_repl: 0,
+        };
+
+        unsafe { pthread_setschedparam(pthread_self(), SCHED_EXT, &param as *const sched_param) }
+    }
+
+    // Pick an idle CPU for the target PID.
+    pub fn select_cpu(&mut self, pid: i32, cpu: i32, flags: u64) -> i32 {
+        let prog = &mut self.skel.progs.rs_select_cpu;
+        let mut args = task_cpu_arg {
+            pid: pid as c_int,
+            cpu: cpu as c_int,
+            flags: flags as c_ulong,
+        };
+        let input = ProgramInput {
+            context_in: Some(unsafe {
+                std::slice::from_raw_parts_mut(
+                    &mut args as *mut _ as *mut u8,
+                    std::mem::size_of_val(&args),
+                )
+            }),
+            ..Default::default()
+        };
+        let out = prog.test_run(input).unwrap();
+
+        out.return_value as i32
+    }
+
+    // Receive a task to be scheduled from the BPF dispatcher.
+    pub fn dequeue_task(&mut self) -> Result<Option<QueuedTask>, i32> {
+        match self.queued.consume_raw() {
+            0 => {
+                self.skel.maps.bss_data.nr_queued = 0;
+                Ok(None)
+            }
+            LIBBPF_STOP => {
+                // A valid task is received, convert data to a proper task struct.
+                let task = unsafe { EnqueuedMessage::from_bytes(&BUF.0).to_queued_task() };
+                let _ = self.skel.maps.bss_data.nr_queued.saturating_sub(1);
+
+                Ok(Some(task))
+            }
+            res if res < 0 => Err(res),
+            res => panic!(
+                "Unexpected return value from libbpf-rs::consume_raw(): {}",
+                res
+            ),
+        }
+    }
+
+    // Send a task to the dispatcher.
+    pub fn dispatch_task(&mut self, task: &DispatchedTask) -> Result<(), libbpf_rs::Error> {
+        // Reserve a slot in the user ring buffer.
+        let mut urb_sample = self
+            .dispatched
+            .reserve(std::mem::size_of::<bpf_intf::dispatched_task_ctx>())?;
+        let bytes = urb_sample.as_mut();
+        let dispatched_task = plain::from_mut_bytes::<bpf_intf::dispatched_task_ctx>(bytes)
+            .expect("failed to convert bytes");
+
+        // Convert the dispatched task into the low-level dispatched task context.
+        let bpf_intf::dispatched_task_ctx {
+            pid,
+            cpu,
+            flags,
+            slice_ns,
+            vtime,
+            cpumask_cnt,
+            ..
+        } = &mut dispatched_task.as_mut();
+
+        *pid = task.pid;
+        *cpu = task.cpu;
+        *flags = task.flags;
+        *slice_ns = task.slice_ns;
+        *vtime = task.vtime;
+        *cpumask_cnt = task.cpumask_cnt;
+
+        // Store the task in the user ring buffer.
+        //
+        // NOTE: submit() only updates the reserved slot in the user ring buffer, so it is not
+        // expected to fail.
+        self.dispatched
+            .submit(urb_sample)
+            .expect("failed to submit task");
+
+        Ok(())
+    }
+
+    // Read exit code from the BPF part.
+    pub fn exited(&mut self) -> bool {
+        self.shutdown.load(Ordering::Relaxed) || uei_exited!(&self.skel, uei)
+    }
+
+    // Called on exit to shutdown and report exit message from the BPF part.
+    pub fn shutdown_and_report(&mut self) -> Result<UserExitInfo> {
+        self.struct_ops.take();
+        uei_report!(&self.skel, uei)
+    }
+}
+
+// Disconnect the low-level BPF scheduler.
+impl<'a> Drop for BpfScheduler<'a> {
+    fn drop(&mut self) {
+        if let Some(struct_ops) = self.struct_ops.take() {
+            drop(struct_ops);
+        }
+        ALLOCATOR.unlock_memory();
+    }
+}
